from flask import Blueprint, render_template, request, redirect, url_for, flash, send_file, current_app, jsonify
from flask_login import login_required, current_user
from models import db
from models_reports import ReportState, ServiceTicket
from utils.decorators import role_required, permission_required
import pandas as pd
import json
import io
import re
from decimal import Decimal
import numpy as np 
from io import BytesIO 
from datetime import datetime 
from time import time

machine_reports_bp = Blueprint('machine_reports_bp', __name__)

# ุฃูุณุงู ุงูุชูุงุฑูุฑ ุงูุนุงูุฉ
CATEGORIES = {
    "bakeries": "ูุฎุงุจุฒ",
    "ration": "ุชูููู",
    "substitute": "ุงูุงุณุชุจุฏุงู",
}

# ูุงุฆูุฉ ุงูุฃุนุทุงู ุงููุณููุญ ุจูุง (ุชุธูุฑ ูู ุงููุงุฆูุฉ ุงูููุณุฏูุฉ ูู ุงููุงุฌูุฉ)
ALLOWED_FAULT_TYPES = [
    "ุฑูุฏุฑ", "ุณููุช", "ุทุจุงุนู", "ุดุญู", "ุณููุช", "ุดุจูู", "ุดุงุดู", "ุจูุช ุดุฑูุญู", "F2", "KEYS", "POWER"
]

# ๐ก ุชู ุงูุชุนุฏูู: ูุถูู ุฃู ููุงุชูุญ "ุงูุนููู" ูู ุงูุฃููููุฉ ุงููุตูู ููุฏูุฌ (ุงูููุงุชูุญ 2 ู 4)
ENTITY_KEYS = {
    "bakeries": [("ุฑูู ุงูุนููู", "ุงุณู ุงูุนููู"), ("ุฑูู ุงููุฎุจุฒ", "ุงุณู ุงููุฎุจุฒ")],
    "default":  [("ุฑูู ุงูุนููู", "ุงุณู ุงูุนููู"), 
                 ("ุฑูู ุงูุชุงุฌุฑ", "ุงุณู ุงูุชุงุฌุฑ"),
                 ("trader_id", "trader_name")]
}

# ููุงุชูุญ ุงูููู ุงูุซุงูุซ
OFFICE_KEYS = ("ุงูุงุฏุงุฑุฉ", "ุงูููุชุจ")
# ุงูุณูุงุญ ุจุณุชุฉ ูููุงุช ูู ุงูุงุณุชูุฑุงุฏ ุจุญุณุจ ุงููุชุทูุจุงุช ุงูุฌุฏูุฏุฉ
MAX_FILES = 6

# ูุงุด ุฎููู ููุชุงุฆุฌ ุชุญููู ุณุฌู ุงูุฒูุงุฑุงุช ูุชูููู ุถุฑุจุงุช ูุงุนุฏุฉ ุงูุจูุงูุงุช ุฎูุงู ูุชุฑุฉ ูุตูุฑุฉ
_VISIT_CACHE = {'df': None, 'meta': None, 'ts': 0}
_VISIT_CACHE_TTL_SEC = 60

# ุงูุฃุนูุฏุฉ ุงูุฎุงุตุฉ ุจุชูุงุตูู ูุณูุณู ุงููุงูููุฉ (ุณุชุธูุฑ ุชุญุช ุจุนุถูุง ูู ุงูุดุงุดุฉ)
MACHINE_DETAIL_COLS = [
    'ูุณูุณู ุงููุงูููุฉ',
    'ูุงูููุฉ ุฑุฆูุณูุฉ/ูุฑุนูุฉ',
    'ุฑูู ุงููุงูููุฉ',
    'ุญุงูุฉ ุงููุงูููุฉ',
    'ุดุฑูุญุฉ 1',  
    'ุดุฑูุญุฉ 2',  
    'ุญุงูุฉ ูุธุงู ุงููุทุญู',
    'SW_AC_SUP',
    'SW_IC_SUP',
    'SW_OD_SUP',
    'POS_VERSION',
    'ุงุณู ุงูุฎุจุฒ',   
    'LOAF_BALANCE1',
    'ุณุงุนุฉ ุจุฏุก ุงูุจูุน',
    'ุณุงุนุฉ ููุงูุฉ ุงูุจูุน',
]

# ๐ก ุงูุฃุนูุฏุฉ ุงูุชู ูุชููุนูุง HTML ูู ุงููุฑุจุน ุงูุฃูู 'ุงูุจูุงูุงุช ุงูุฃุณุงุณูุฉ'
CUSTOMER_DETAIL_COLS = [
     'ุงูุงุฏุงุฑุฉ', 'ุงูููุชุจ', 'ุฑูู ุงูุนููู', 'ุงุณู ุงูุนููู', 'ุงุณู ุงููุณุคู',
     'ุงูุฑูู ุงููููู', 'ุฑูู ุงููุญููู', 'ุฑูู ูุงุชู', 'ุญุงูุฉ ุงูุนููู', 'ููุงุญุธุงุช'
]

# ==================== (1) ุงูุฏูุงู ุงููุณุงุนุฏุฉ ุงูุนุงูุฉ (Utils, Coercion, State) ====================

# ========== ุชุญููู ูู ุงูููู ููุต + ุฅุฒุงูุฉ ุงูููุช + ุชูุธูู nan/none ==========
_EMPTY_TOKENS = {"nan", "none", "null", "na", "n/a", "nat", "-", "โ"}
_DATE_RE_1 = re.compile(r"^(\d{4}[-/]\d{1,2}[-/]\d{1,2})[ T]\d{1,2}:\d{2}(?::\d{2})?$")
_DATE_RE_2 = re.compile(r"^(\d{1,2}[-/]\d{1,2}[-/]\d{4})[ T]\d{1,2}:\d{2}(?::\d{2})?$")

def _strip_time_from_date(txt: str) -> str:
    m = _DATE_RE_1.match(txt)
    if m: return m.group(1)
    m = _DATE_RE_2.match(txt)
    if m: return m.group(1)
    return txt

def _textify(v) -> str:
    """ุชุญููู ุงููููุฉ ุฅูู ูุต ูุธูู ูุน ูุนุงูุฌุฉ ุงูุฃุฑูุงู ุงูุนุดุฑูุฉ ูุงูุฃุณูุฉ ุจุชุจุณูุท."""
    if v is None: return ""
    
    if isinstance(v, (float, np.float64)):
        if np.isinf(v):
            return "" 
        if np.isnan(v):
            return "" 
    
    t = str(v).strip()
    if t == "" or t.lower() in _EMPTY_TOKENS: return ""
    
    t = t.replace(",", "")
    # ูุง ูููู ุจุฅุฒุงูุฉ ููุช ุงูุชุงุฑูุฎ ุจุนุฏ ุงูุขู ูุถูุงู ุธููุฑ ููุช ุงูุฒูุงุฑุฉ ูู ุงููุงุฌูุฉ

    # ๐ก FIX: ุชุฌูุจ ุชุญููู ุฃุฑูุงู ุงูุชุนุฑูู ุงูุทูููุฉ (ูุซู ุงููุณูุณูุงุช ุฃู ุงูุดุฑุงุฆุญ) ุฅูู float ูุชุฌูุจ ููุฏุงู ุงูุฏูุฉ
    # ุฅุฐุง ูุงู ุงููุต ูุชููู ูู ุฃุฑูุงู ููุท ูุทููู ุฃูุจุฑ ูู 12ุ ูุงุญูุธู ููุต (String).
    if t.isdigit() and len(t) > 12: 
        return t 
        
    try:
        float_val = float(t)
        
        if np.isinf(float_val) or np.isnan(float_val):
            return ""
            
        if float_val == int(float_val):
            return str(int(float_val))
        
        # ูุญุงููุฉ ุงูุชุนุงูู ูุน ุงูุฃุฑูุงู ุงูุนุดุฑูุฉ ุงููุจูุฑุฉ ุจุทุฑููุฉ Decimal
        try:
             # ุชุญููููุง ุฅูู ูููุฉ ุตุญูุญุฉ ููุฑุจุฉ (ุฅู ุฃููู) ูุชุฌูุจ ุงูุฃุตูุงุฑ ุงูุนุดุฑูุฉ ุบูุฑ ุงูุถุฑูุฑูุฉ
             # ุฃู ุชุฑููุง ููููุฉ ุนุดุฑูุฉ ูุจุณุทุฉ ุฅู ูู ููู ุฑููุงู ุตุญูุญุงู
             return str(float_val) 
        except Exception:
             pass

    except ValueError:
        pass 

    return t


def _coerce_text_df(df: pd.DataFrame) -> pd.DataFrame:
    # ๐ก ุชู ุงูุชุนุฏูู ููุง: ุฅุถุงูุฉ ุชุญูู ุตุฑูุญ ุจุงุณุชุฎุฏุงู isinstance ูุญู ูุดููุฉ 'function' object has no attribute 'empty'
    if df is None or not isinstance(df, pd.DataFrame) or df.empty: 
        return pd.DataFrame()
        
    out = df.copy()
    
    # 1. ุชูุธูู ุฃุณูุงุก ุงูุฃุนูุฏุฉ
    out.columns = [str(c).strip().replace("\n"," ").replace("\r"," ") for c in out.columns]
    
    # 2. ุถูุงู ุชูุฑุฏ ุฃุณูุงุก ุงูุฃุนูุฏุฉ (ูุฏ ุชููู ุงูุฃุนูุฏุฉ ุงูููุฑุฑุฉ ุชุณุจุจ ูุดุงูู)
    seen = set()
    new_cols = []
    for col in out.columns:
        if col in seen:
            continue
        seen.add(col)
        new_cols.append(col)
        
    out = out[new_cols] 
    
    # 3. ุชุทุจูู _textify
    for c in out.columns:
        out[c] = out[c].map(_textify)
        
    return out

# ========== State (ุงูุญุงูุฉ) ==========
def _ensure_tables():
    """ูุธููุฉ ุชุฃูุฏ ูู ูุฌูุฏ ุงูุฌุฏุงูู (ูู ReportState)"""
    # ๐ก ููุงุญุธุฉ: ูุง ูุฌุจ ุงุณุชุฏุนุงุก db.create_all() ููุง ุจู ูู ููุทุฉ ุชููุฆุฉ ุงูุชุทุจูู
    # ูููู ุชู ุชุฑููุง ูุคูุชุงู ูุชุฌูุจ ูุดู ุฅุฐุง ูู ููู ููุงู ุชููุฆุฉ ุนุงูุฉ.
    try: db.create_all() 
    except Exception: pass

def _df_to_json(df: pd.DataFrame) -> str:
    return _coerce_text_df(df).to_json(orient="records", force_ascii=False)


def _json_to_df(js: str) -> pd.DataFrame:
    if not js: return pd.DataFrame()
    df = pd.DataFrame(json.loads(js))
    return _coerce_text_df(df)

def _load_state(category: str):
    """
    ุชุญููู ุณุฌู ุญุงูุฉ ุงูุชูุฑูุฑ ูููุฆุฉ ุงููุญุฏุฏุฉ.
    - ุฃููููุฉ: ุญุงูุฉ ูุฑุชุจุทุฉ ุจุงููุณุชุฎุฏู ุงูุญุงูู.
    - ุณููุท ุงุญุชูุงุทู: ุญุงูุฉ ุนุงูุฉ ุจุฏูู user_id ุฅุฐุง ูู ุชูุฌุฏ ุญุงูุฉ ุงููุณุชุฎุฏู.
    """
    _ensure_tables()
    try:
        # ุฃูููุง: ุญุงูุฉ ุงููุณุชุฎุฏู ุฅู ูุงู ูุณุฌูููุง
        if current_user.is_authenticated:
            row = ReportState.query.filter_by(category=category, user_id=current_user.id).first()
            if row:
                return row
        # ุซุงูููุง: ุณููุท ุงุญุชูุงุทู ุนูู ุญุงูุฉ ุนุงูุฉ (ูุฏ ุชููู ูุญููุธุฉ ูู ุฎุฏูุงุช ุงูุชุฌุงุฑ ุจุฏูู user_id)
        # ูุฃุฎุฐ ุฃุญุฏุซ ุณุฌู ุจููุณ ุงููุฆุฉ ุจุบุถ ุงููุธุฑ ุนู user_id
        return (ReportState.query
                .filter(ReportState.category == category)
                .order_by(ReportState.id.desc())
                .first())
    except Exception:
        return None

def _save_state(category: str, df: pd.DataFrame = None, mapping: dict = None):
    """
    ุญูุธ ุณุฌู ุญุงูุฉ ุงูุชูุฑูุฑ ุงูุฎุงุต ุจุงููุณุชุฎุฏู ุงูุญุงูู ูุงููุฆุฉ ุงููุญุฏุฏุฉ.
    """
    if not current_user.is_authenticated:
        return
        
    # ๐ก ุชู ุงูุชุนุฏูู: ูุณุชุฎุฏู user_id
    row = ReportState.query.filter_by(category=category, user_id=current_user.id).first()
    if not row:
        row = ReportState(category=category, user_id=current_user.id)
        db.session.add(row)
        
    if df is not None:
        row.data_json = _df_to_json(df)
    if mapping is not None:
        row.mapping_json = json.dumps(mapping, ensure_ascii=False)
        
    db.session.commit()

def _apply_mapping(df: pd.DataFrame, mapping: dict) -> pd.DataFrame:
    """ุชุทุจูู ุฅุนุงุฏุฉ ุชุณููุฉ ูุชุฑุชูุจ ุงูุฃุนูุฏุฉ"""
    if df is None or df.empty or not mapping:
        return df
    rename = mapping.get("rename") or {}
    order  = [c for c in (mapping.get("order") or []) if c]
    out = df.copy()
    if rename: out = out.rename(columns=rename)
    if order:
        front = [c for c in order if c in out.columns]
        # ุฅุธูุงุฑ ุงูุฃุนูุฏุฉ ุงููุญููุธุฉ ููุท ูุฅุฎูุงุก ุงูุจุงูู ูู ุงูุนุฑุถ/ุงูุชุตุฏูุฑ
        out = out[front] if front else out
    return _coerce_text_df(out)

def _drop_empty_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ุฅุฒุงูุฉ ุงูุฃุนูุฏุฉ ุงูุชู ูุง ุชุญุชูู ุนูู ุฃู ุจูุงูุงุช"""
    if df is None or df.empty: return df
    
    def _is_empty_series(s: pd.Series) -> bool:
        if not isinstance(s, pd.Series):
             return s.empty if hasattr(s, 'empty') else True
             
        vals = s.fillna("").astype(str).str.strip().str.lower()
        return ((vals == "") | (vals.isin(_EMPTY_TOKENS))).all()
    
    keep = [c for c in df.columns if not _is_empty_series(df[c])]
    return df[keep] if keep else df.iloc[:, 0:0]

def _filter_dataframe(df: pd.DataFrame, query: str, search_cols: list[str] | None = None) -> pd.DataFrame:
    """ุชุตููุฉ ุณุฑูุนุฉ ูุน ุชูุญูุฏ ุนุฑุจูุ ุจุฃูู ุนุฏุฏ ูู ุงูุนูููุงุช.
    - ุชุฌูุน ูุต ุงูุตู ูุฑุฉ ูุงุญุฏุฉ ุนุจุฑ ุงูุฃุนูุฏุฉ ุงููุณุชูุฏูุฉ ุซู ุชุทุจู contains.
    - ุชููู ุงูุชูููุฉ ูู O(rows * cols) ุฅูู O(rows).
    """
    if not query:
        return df

    q = _norm_key_text(query).lower()
    if not q:
        return df

    # ุงูุฃุนูุฏุฉ ุงููุณุชูุฏูุฉ ููุชุตููุฉ
    cols_to_search = (
        [c for c in (search_cols or []) if c in df.columns]
        if (search_cols and isinstance(search_cols, list)) else list(df.columns)
    )

    if not cols_to_search:
        return pd.DataFrame(columns=df.columns)

    # ุงููุณุงุฑ ุงูุณุฑูุน: ุชุฌููุน ูุต ุงูุตู ูุฑุฉ ูุงุญุฏุฉ ุซู ุงูุชูุญูุฏ ูุงูุจุญุซ
    try:
        # ุชุญููู ุฅูู ูุต ูุชุนุจุฆุฉ ุงููุฑุงุบุงุช ุซู ุชุฌููุน ุงูุตููู
        rows_text = df[cols_to_search].fillna("").astype(str)
        # ุชุฌููุน ูุต ุงูุตู ูุฑุฉ ูุงุญุฏุฉ
        all_text = rows_text.apply(lambda r: " ".join(r.values.tolist()), axis=1)
        # ุชูุญูุฏ ุนุฑุจู + ุชุตุบูุฑ ุซู contains
        all_text_norm = all_text.map(_norm_key_text).str.lower()
        mask = all_text_norm.str.contains(q, na=False)
        return df[mask]
    except Exception:
        # ูุณุงุฑ ุงุญุชูุงุทู: ููุณ ุงููููุฌ ุงูุณุงุจู ุนููุฏูุง ุจุนููุฏ
        try:
            df_text_normalized = df[cols_to_search].astype(str).apply(
                lambda col: col.map(lambda v: _norm_key_text(v).lower())
            )
            mask = df_text_normalized.apply(lambda col: col.str.contains(q, na=False))
            return df[mask.any(axis=1)]
        except Exception:
            # ูู ุญุงู ุญุฏูุซ ุฎุทุฃ ุบูุฑ ูุชููุนุ ุฃุนุฏ ุงูุฅุทุงุฑ ููุง ูู ูุชุฌูุจ ูุณุฑ ุงููุงุฌูุฉ
            return df


# ==================== (2) ูุธุงุฆู ุงูุจุญุซ ูุงูุชุฌููุน ====================

def _get_entity_grouping_keys(filtered_df: pd.DataFrame, category: str) -> list:
    """ูุญุฏุฏ ุฃุนูุฏุฉ ุฑูู ูุงุณู ุงูููุงู (ุงูุนููู/ุงููุฎุจุฒ/ุงูุชุงุฌุฑ) ูุงุณุชุฎุฏุงููุง ูููุงุชูุญ ุชุฌููุน ุจุดูู ุฃูุซุฑ ูุฑููุฉ."""

    df_cols = list(filtered_df.columns)

    # ุฃุฒูุงุฌ ููุงุชูุญ ูุญุชููุฉ ุจุญุณุจ ุงููุฆุฉ ูุน ุชุถููู ุงููุฑุงุฏูุงุช ุงูุนุฑุจูุฉ ูุงูุฅูุฌููุฒูุฉ
    candidate_pairs: list[tuple[str, str]] = []
    if category == 'bakeries':
        candidate_pairs.extend([
            ('ุฑูู ุงูุนููู', 'ุงุณู ุงูุนููู'),
            ('ุฑูู ุงููุฎุจุฒ', 'ุงุณู ุงููุฎุจุฒ'),
            ('ุฑูู ุงูุชุงุฌุฑ', 'ุงุณู ุงูุชุงุฌุฑ'),
        ])
    elif category in ['ration', 'substitute']:
        candidate_pairs.extend([
            ('ุฑูู ุงูุนููู', 'ุงุณู ุงูุนููู'),
            ('ุฑูู ุงูุชุงุฌุฑ', 'ุงุณู ุงูุชุงุฌุฑ'),
        ])
    else:
        candidate_pairs.extend([
            ('ุฑูู ุงูุนููู', 'ุงุณู ุงูุนููู'),
            ('ุฑูู ุงูุชุงุฌุฑ', 'ุงุณู ุงูุชุงุฌุฑ'),
        ])

    # ุฅุถุงูุฉ ูุฑุงุฏูุงุช ุนุงูุฉ
    candidate_pairs.extend([
        ('Customer Code', 'Customer Name'),
        ('Customer ID', 'Customer Name'),
        ('Customer_ID', 'Customer Name'),
        ('Trader ID', 'Trader Name'),
        ('trader_id', 'trader_name'),
        ('ุฑูู ุงููุฎุจุฒ', 'ุงุณู ุงููุฎุจุฒ'),
    ])

    # ุงุฎุชุฑ ุฃูู ุฒูุฌ ููุฌูุฏ ุจุงููุงูู ูู ุงูุฃุนูุฏุฉ
    for code_key, name_key in candidate_pairs:
        if (code_key in df_cols) and (name_key in df_cols):
            return [code_key, name_key]

    # ูุญุงููุฉ ุฃุฎูุฑุฉ: ุงูุชุดุงู ุฃู ุนููุฏ ูุญุชูู "ุฑูู" ูุฃู ุนููุฏ ูุญุชูู "ุงุณู"
    code_key = next((c for c in df_cols if ('ุฑูู' in c) or (c.lower() in {'customer code','customer id','customer_id','trader id','trader_id'})), None)
    name_key = next((c for c in df_cols if ('ุงุณู' in c) or (c.lower() in {'customer name','trader name','trader_name'})), None)
    keys = []
    if code_key: keys.append(code_key)
    if name_key and name_key != code_key: keys.append(name_key)
    if keys:
        return keys

    # Fallback: ุฃูู ุนููุฏูู ุฅุฐุง ูู ูุชู ุงูุนุซูุฑ ุนูู ููุงุชูุญ ููุงุณุจุฉ
    return list(df_cols)[:2]

# ๐ก ุชู ุชุญุฏูุซ MACHINE_DATA_SOURCE_MAPPING ูุถูุงู ุดููู ุจุฏุงุฆู ุงูุดุฑุงุฆุญ
MACHINE_DATA_SOURCE_MAPPING = {
    'ูุณูุณู ุงููุงูููุฉ': ['ูุณูุณู ุงููุงูููุฉ', 'ูุณูุณู', 'Serial'], 
    'ุฑูู ุงููุงูููุฉ': ['ุฑูู ุงููุงูููุฉ', 'ููุฏ ุงููุงูููุฉ', 'Machine Code'],
    'ูุงูููุฉ ุฑุฆูุณูุฉ/ูุฑุนูุฉ': ['ูุงูููุฉ ุฑุฆูุณูุฉ/ูุฑุนูุฉ', 'ุงูููุน', 'Type'], 
    'ุญุงูุฉ ุงููุงูููุฉ': ['ุญุงูุฉ ุงููุงูููุฉ', 'ุงูุญุงูุฉ', 'Status'],
    # ุฅูุบุงุก ุงุณุชุฎุฏุงู CS_1/CS_2 ูุจุฏุงุฆู ูุฃููุง ููุณุช ุฃุฑูุงู ุดุฑุงุฆุญ ูุนููุฉ
    'ุดุฑูุญุฉ 1': ['ุดุฑูุญุฉ 1', 'ุดุฑูุญุฉ1', 'SIM1'], 
    'ุดุฑูุญุฉ 2': ['ุดุฑูุญุฉ 2', 'ุดุฑูุญุฉ2', 'SIM2'], 
}
MACHINE_CODE_COL = 'ุฑูู ุงููุงูููุฉ'
MACHINE_SERIAL_COL = 'ูุณูุณู ุงููุงูููุฉ' 
SLICE_COLS = ['ุดุฑูุญุฉ 1', 'ุดุฑูุญุฉ 2']

# ========== ูุธุงุฆู ูุณุงุนุฏุฉ ูุงุฎุชูุงุฑ ููุงุชูุญ ุงูุฏูุฌ ุงููุฑูุฉ (FIX ููุดุฑุงุฆุญ) ==========

def _find_actual_col(standard_col_name: str, df_cols: list) -> str | None:
    """ูุญุฏุฏ ุงูุงุณู ุงููุนูู ููุนููุฏ ุจูุงุกู ุนูู ูุงุฆูุฉ ุงูุฃุณูุงุก ุงูุจุฏููุฉ (ูู MACHINE_DATA_SOURCE_MAPPING) ูุงูุฃุนูุฏุฉ ุงูููุฌูุฏุฉ ูู DataFrame."""
    # ุงุณุชุฎุฏุงู ุงูุงุณู ุงูููุงุณู ููุณู ูุฃู ุจุฏุงุฆู ูุญุฏุฏุฉ ูู
    source_names = MACHINE_DATA_SOURCE_MAPPING.get(standard_col_name, [standard_col_name])
    
    # ุงูุฃููููุฉ: ุงูุงุณู ุงูููุงุณู ุฃููุงูุ ุซู ุงูุจุฏุงุฆู
    for name in source_names:
        if name in df_cols:
            return name
    return None

def _pick_machine_keys(df1_cols: list, df2_cols: list) -> list[str]:
    """ูุญุฏุฏ ููุงุชูุญ ุงููุงูููุฉ ุงููุดุชุฑูุฉ (ูุณูุณู/ุฑูู) ููุฏูุฌ ุจุงูุงุนุชูุงุฏ ุนูู ุฃุณูุงุก ุงูุฃุนูุฏุฉ ุงููุนููุฉ."""
    
    # ูุญุงููุฉ ุฅูุฌุงุฏ ุงุณู ุงูุนููุฏ ุงููุนูู ูู 'ูุณูุณู ุงููุงูููุฉ' ูู ูู ูู DF1 ู DF2
    serial1 = _find_actual_col(MACHINE_SERIAL_COL, df1_cols)
    serial2 = _find_actual_col(MACHINE_SERIAL_COL, df2_cols)
    
    # ูุญุงููุฉ ุฅูุฌุงุฏ ุงุณู ุงูุนููุฏ ุงููุนูู ูู 'ุฑูู ุงููุงูููุฉ' ูู ูู ูู DF1 ู DF2
    code1 = _find_actual_col(MACHINE_CODE_COL, df1_cols)
    code2 = _find_actual_col(MACHINE_CODE_COL, df2_cols)

    # ุงูุฃููููุฉ: ูุณูุณู ุงููุงูููุฉ
    if serial1 and serial2 and _norm_key_text(serial1) == _norm_key_text(serial2):
         return [serial1]
    
    # ุงูุฃููููุฉ ุงูุชุงููุฉ: ุฑูู ุงููุงูููุฉ
    if code1 and code2 and _norm_key_text(code1) == _norm_key_text(code2):
         return [code1]
         
    return [] # ูุง ููุฌุฏ ููุชุงุญ ูุงูููุฉ ูุดุชุฑู ูููู ุงุณุชุฎุฏุงูู


def _group_search_results(filtered_df: pd.DataFrame, category: str) -> list[dict]:
    """ุชุฌููุน ุณุฌูุงุช DataFrame"""
    if filtered_df.empty: return []

    group_keys = _get_entity_grouping_keys(filtered_df, category)
    
    valid_group_keys = [k for k in group_keys if k in filtered_df.columns]
    
    if len(valid_group_keys) < 2:
        # ุฅุฐุง ูู ูุชู ุงูุนุซูุฑ ุนูู ููุงุชูุญ ููุงู ุซูุงุฆูุฉ (ุฑูู ูุงุณู)ุ ูุนุชุจุฑ ูู ุตู ููุงูุงู ูุณุชููุงู
        grouped = [(i, filtered_df.iloc[[i]]) for i in range(len(filtered_df))]
        valid_group_keys = []
    else:
        grouped = filtered_df.groupby(valid_group_keys, dropna=False).__iter__()

    result_list = []
    
    EXCLUDE_FROM_COMMON = [
        # ุฃุนูุฏุฉ ุงููุณูุณูุงุช
        *MACHINE_DETAIL_COLS, 
        # ุฃุนูุฏุฉ ูุฌุจ ุงุณุชุจุนุงุฏูุง ูู ุงูุจูุงูุงุช ุงููุดุชุฑูุฉ ูุชุธูุฑ ูู ููุงู ุขุฎุฑ ุฃู ุบูุฑ ุถุฑูุฑูุฉ
        'CS_3', 'COUNT_DIST', 'LOAF_BALANCE', 
        'timestamp', 'report_data',
        'CS_1', 'CS_2', # ูุชู ุงุณุชุจุนุงุฏูุง ูุฃููุง ุฃุตุจุญุช ุจุฏุงุฆู ูู ุดุฑูุญุฉ 1 ู ุดุฑูุญุฉ 2
    ] 
    
    for _, group_df in grouped:
        first_record = group_df.iloc[0].to_dict()
        common_data = {}
        
        # ุงุณุชุฎุฑุงุฌ ุงูุจูุงูุงุช ุงููุดุชุฑูุฉ (ุชุธูุฑ ูุฑุฉ ูุงุญุฏุฉ ููููุงู)
        for col in group_df.columns:
              if col in first_record and col not in EXCLUDE_FROM_COMMON:
                  common_data[col] = first_record[col]

        # ๐ก FIX: ููุทู ุงุณุชุฎูุงุต ุงูุดุฑุงุฆุญ ุนุจุฑ ููุฏ ุงููุงูููุฉ (ุฑูู ุงููุงูููุฉ)
        # 1. ุฅูุดุงุก ุฌุฏูู ุจุญุซ ููุดุฑุงุฆุญ ุจูุงุกู ุนูู ุฑูู ุงููุงูููุฉ ูู ุงููุฌููุนุฉ ุจุฃููููุง
        slice_lookup = {}
        # ุชุญุฏูุฏ ุงุณู ุนููุฏ "ุฑูู ุงููุงูููุฉ" ุงููุนูู ูู ุจูุงูุงุช ุงูุนููู (Group_df)
        actual_machine_code_col = next((c for c in MACHINE_DATA_SOURCE_MAPPING[MACHINE_CODE_COL] if c in group_df.columns), MACHINE_CODE_COL)

        for _, r in group_df.iterrows():
            machine_code = _textify(r.get(actual_machine_code_col))
            if machine_code:
                row_slices = {}
                for slice_col in SLICE_COLS:
                    value = r.get(slice_col) # 1. ุชุญูู ูู ุงูุงุณู ุงูุฃุณุงุณู
                    
                    # 2. ุชุญูู ูู ุฃุณูุงุก ุงูุฃุนูุฏุฉ ุงูุจุฏููุฉ ูู ููุณ ุงูุตู
                    if _textify(value) == '':
                        for alt_col in MACHINE_DATA_SOURCE_MAPPING.get(slice_col, []):
                            alt_value = r.get(alt_col)
                            if _textify(alt_value) != '':
                                value = alt_value
                                break
                                
                    if _textify(value) != '':
                         row_slices[slice_col] = value

                # ุฏูุฌ ุงููุชุงุฆุฌ: ุงูุฃููููุฉ ูููููุฉ ุบูุฑ ุงููุงุฑุบุฉ ุงูููุชุดูุฉ ุฃููุงู
                if machine_code not in slice_lookup:
                     slice_lookup[machine_code] = row_slices
                else:
                     for k, v in row_slices.items():
                          if _textify(v) != '':
                               slice_lookup[machine_code][k] = v


        # ุงุณุชุฎุฑุงุฌ ุชูุงุตูู ุงููุงูููุงุช
        machine_details = []
        for _, row in group_df.iterrows():
            detail = {}
            
            current_machine_code = _textify(row.get(actual_machine_code_col))
            
            for col in MACHINE_DETAIL_COLS:
                # 1. ุงููููุฉ ุงูุฃุณุงุณูุฉ
                value = row.get(col) or '-' 
                
                # 2. ููุทู ุงูุดุฑุงุฆุญ: ุงูุจุญุซ ูู ุฌุฏูู ุงูู Lookup ุฅุฐุง ูุงูุช ุงููููุฉ ููููุฏุฉ
                if col in SLICE_COLS:
                    # ุฅุฐุง ูุงูุช ุงููููุฉ ุงูุฃุณุงุณูุฉ ูุงุฑุบุฉุ ูุจุญุซ ูู ุฌุฏูู ุงูู Lookup ุงููุฌููุน
                    if _textify(value) == '' and current_machine_code and current_machine_code in slice_lookup:
                        lookup_value = slice_lookup[current_machine_code].get(col)
                        if _textify(lookup_value) != '':
                             value = lookup_value
                             
                # 3. ุงูุจุญุซ ูู ุงูุจุฏุงุฆู ุงูุฃุฎุฑู (ูุดูู ุงููุณูุณู ูุฑูู ุงููุงูููุฉ ูุบูุฑููุง)
                if _textify(value) == '' and col in MACHINE_DATA_SOURCE_MAPPING:
                    for alt_col in MACHINE_DATA_SOURCE_MAPPING[col]:
                        alt_value = row.get(alt_col)
                        if _textify(alt_value) != '':
                            value = alt_value
                            break
                            
                # 4. ุญูุธ ุงููููุฉ ุงูููุงุฆูุฉ
                detail[col] = _textify(value) or '-'
                
            machine_details.append(detail)
            
        result_list.append({
            'common_data': common_data,
            'machine_details': machine_details,
            'group_keys': valid_group_keys,
        })

    return result_list

# ==================== (3) ุงูุฏูุงู ุงููุณุงุนุฏุฉ ูุฌูุจ ุงูุจูุงูุงุช ุงูุฎุงุฑุฌูุฉ - ูุญุงูุงุฉ ูุคูุชุฉ ====================

def _fetch_visit_data(customer_code: str, visit_history_df: pd.DataFrame, visit_period: str = 'month', month_label: str | None = None, year_label: str | None = None) -> dict:
    """
    ุญุณุงุจ ุนุฏุฏ ุงูุฒูุงุฑุงุช ููุดูุฑ ุงูุญุงูู ูุงูุณูุฉ ุงูุญุงููุฉ ุจุดูู ูููุตู ุงุนุชูุงุฏูุง ุนูู ุนููุฏ ุงูุชุงุฑูุฎ ุงูููุญูุฏ 'ุงูุชุงุฑูุฎ'.
    - ุฅุฐุง ุชููุฑ ุงูุชุงุฑูุฎ: ูุชู ุงูุนุฏ ุจุฏูุฉ ููู ูุชุฑุฉุ ูุน ุชูุงุตูู ุญุณุจ 'ูุณูุณู' ุฅู ููุฌุฏุช.
    - ุฅุฐุง ูู ูุชููุฑ ุงูุชุงุฑูุฎ: ูุชู ุฅุฑุฌุงุน ุฅุฌูุงูู ูุงุญุฏ ูููุณุชุฎุฏู ูููุชุง ุงููุชุฑุชูู (Fallback).
    """
    if visit_history_df is None or visit_history_df.empty:
        return {'current_month': {'total': 0, 'details': {}}, 'current_year': {'total': 0, 'details': {}}}

    df = visit_history_df.copy()
    now = datetime.now()

    # ูุญุงููุฉ ุงุณุชุฎุฑุงุฌ ุงูุชุงุฑูุฎ ุฅุฐุง ูุงู ููุฌูุฏูุง
    has_date = 'ุงูุชุงุฑูุฎ' in df.columns
    has_serial = 'ูุณูุณู' in df.columns

    if has_date:
        # ุชุญููู ุงูุชุงุฑูุฎ ุฅูู datetime ุจุฃูุงู
        try:
            dates = pd.to_datetime(df['ุงูุชุงุฑูุฎ'], errors='coerce', dayfirst=True)
        except Exception:
            dates = pd.to_datetime(df['ุงูุชุงุฑูุฎ'].astype(str), errors='coerce', dayfirst=True)
        df['_dt'] = dates
        # ุจูุงุก ุฃููุนุฉ ุงููุชุฑุฉ
        # ุฏุนู ุงุฎุชูุงุฑ ุดูุฑ/ุณูุฉ ูุญุฏุฏูู ูู ุชุณููุงุช ุงููููุงุช (_ุงููุชุฑุฉ) ุฅุฐุง ูุงูุช ูุชููุฑุฉ
        if month_label and isinstance(month_label, str) and len(month_label) == 7:
            try:
                target_year = int(month_label.split('-')[0])
                target_month = int(month_label.split('-')[1])
            except Exception:
                target_year = now.year
                target_month = now.month
        else:
            target_year = now.year
            target_month = now.month

        if year_label and isinstance(year_label, str) and len(year_label) == 4 and year_label.isdigit():
            try:
                target_year_for_year = int(year_label)
            except Exception:
                target_year_for_year = now.year
        else:
            target_year_for_year = target_year

        month_mask = (df['_dt'].dt.year == target_year) & (df['_dt'].dt.month == target_month)
        year_mask  = (df['_dt'].dt.year == target_year_for_year)

        # ุงุฎุชูุงุฑ ูุฌููุนุงุช ุจุญุณุจ ุงููุชุฑุฉ ุงููุทููุจุฉ
        if visit_period == 'year':
            # ุนูุฏ ุงุฎุชูุงุฑ ุงูุณูุฉุ ูุง ููููู ุญุณุงุจ ุงูุดูุฑ: ุฅุฐุง ุชููุฑุช ุชุณููุฉ ุดูุฑ ุฃู ุฃููู ุงุดุชูุงููุง ูู ุงูุชุงุฑูุฎ
            if '_ุงููุชุฑุฉ' in df.columns and month_label:
                df_month = df[df['_ุงููุชุฑุฉ'] == month_label]
            else:
                df_month = df[(df['_dt'].dt.year == target_year) & (df['_dt'].dt.month == target_month)]
            df_year  = df[year_mask]
        elif visit_period == 'recent_program':
            # ุงูุจูุงูุงุช ุงูุญุฏูุซุฉ: ุงุนุชุจุฑ ูู ุงูุตููู ุญุฏูุซุฉ ุถูู ููุญุฉ ูุงุญุฏุฉ
            df_month = df
            df_year  = df.iloc[0:0]
        else:  # ุงูุงูุชุฑุงุถู 'month'
            # ุฅุฐุง ูุงูุช ูุฏููุง ุฃุนูุฏุฉ ุชุณููุงุช ุงููุชุฑุงุชุ ูุนุชูุฏ ุชุณููุฉ ุงูููู ูุจุงุดุฑุฉ ููุดูุฑ ุงููุญุฏุฏ
            if '_ุงููุชุฑุฉ' in df.columns and month_label:
                df_month = df[df['_ุงููุชุฑุฉ'] == month_label]
            else:
                df_month = df[month_mask]
            df_year  = df[year_mask]

        month_total = int(df_month.shape[0])
        year_total  = int(df_year.shape[0])

        # ุญุณุงุจ ุขุฎุฑ ููุช ุฒูุงุฑุฉ ูุชุงุญ ุฏุงุฎู ุงููุชุฑุฉ ุงููุฎุชุงุฑุฉ (ุงูุดูุฑ/ุงูุญุฏูุซ)
        latest_dt_str = ""
        latest_by_serial = {}
        latest_serial = ""
        try:
            valid_month = df_month[df_month['_dt'].notna()] if ('_dt' in df_month.columns) else df_month
            if not valid_month.empty and ('ุงูุชุงุฑูุฎ' in valid_month.columns):
                idx_latest = valid_month['_dt'].idxmax() if ('_dt' in valid_month.columns) else valid_month.index[-1]
                # ุญูุธ ุงููุต ุงูุฃุตูู ููุชุงุฑูุฎ ุญุชู ูู ูุงู ูุญุชูู ุนูู ุงูููุช
                latest_dt_str = str(valid_month.at[idx_latest, 'ุงูุชุงุฑูุฎ']).strip()
                # ุญูุธ ุงููุณูุณู ุงููุฑุชุจุท ุจุขุฎุฑ ุณุฌู (ุฅู ููุฌุฏ)
                try:
                    if has_serial and ('ูุณูุณู' in valid_month.columns):
                        latest_serial = str(valid_month.at[idx_latest, 'ูุณูุณู']).strip()
                except Exception:
                    pass
                # ุญุณุงุจ ุขุฎุฑ ููุช ููู ูุณูุณู ุฅุฐุง ุชูููุฑ ุนููุฏ ุงููุณูุณู
                if has_serial:
                    try:
                        for serial, sub in valid_month.groupby('ูุณูุณู'):
                            sub_valid = sub[sub['_dt'].notna()] if ('_dt' in sub.columns) else sub
                            if not sub_valid.empty:
                                idx_s = sub_valid['_dt'].idxmax() if ('_dt' in sub_valid.columns) else sub_valid.index[-1]
                                latest_by_serial[serial] = str(sub_valid.at[idx_s, 'ุงูุชุงุฑูุฎ']).strip()
                    except Exception:
                        pass
        except Exception:
            latest_dt_str = latest_dt_str or ""

        if has_serial:
            month_details = df_month.groupby('ูุณูุณู').size().to_dict()
            year_details  = df_year.groupby('ูุณูุณู').size().to_dict()
        else:
            month_details = {}
            year_details  = {}

        return {
            'current_month': {'total': month_total, 'details': month_details},
            'current_year':  {'total': year_total,  'details': year_details},
            'latest_datetime': latest_dt_str,
            'latest_serial': latest_serial,
            'latest_serial_times': latest_by_serial
        }
    else:
        # ูุง ููุฌุฏ ุชุงุฑูุฎ: ุฅุฌูุงูู ูุงุญุฏ ููุณุชุฎุฏู ูููุง ุงููุชุฑุชูู
        total_count = int(df.shape[0])
        if has_serial:
            serial_counts = df.groupby('ูุณูุณู').size().to_dict()
        else:
            serial_counts = {}
        return {
            'current_month': {'total': total_count, 'details': serial_counts},
            'current_year':  {'total': total_count, 'details': serial_counts}
        }

# ==================== ุฅุนุงุฏุฉ ุจูุงุก ููุทู ุงูุฒูุงุฑุงุช: ุชูุญูุฏ ุงูุฃุนูุฏุฉ ูุงููุทุงุจูุฉ ====================
def _standardize_visit_df(df: pd.DataFrame) -> pd.DataFrame:
    """
    ุชูุญูุฏ ุฃุนูุฏุฉ ุณุฌู ุงูุฒูุงุฑุงุช ุฅูู ุฃุณูุงุก ููุงุณูุฉ:
    - ุงูุชุงุฑูุฎ โ 'ุงูุชุงุฑูุฎ'
    - ุงููุณูุณู โ 'ูุณูุณู'
    - ุฑูู ุงูุนููู โ 'ุฑูู ุงูุนููู'
    - ุงุณู ุงูุนููู โ 'ุงุณู ุงูุนููู'
    ูุฏุนู ุฃุณูุงุก ุจุฏููุฉ ุดุงุฆุนุฉ ูููุจูู ุงูุฃุนูุฏุฉ ุงูุฃุฎุฑู ููุง ูู.
    """
    if df is None or df.empty:
        return pd.DataFrame()
    out = df.copy()
    cols = list(out.columns)
    # ุชูุณูุน ุงููุฑุดุญุงุช ูุชุบุทูุฉ ุจุฏุงุฆู ุฃูุซุฑ ู ุงูุจุญุซ ุจุงูุงุดุชูุงู
    date_cands = [
        'ุงูุชุงุฑูุฎ', 'ุชุงุฑูุฎ ุงูุฒูุงุฑุฉ', 'Date', 'Visit Date', 'ุชุงุฑูุฎ',
        'visit_date', 'date', 'created_at', 'dt'
    ]
    # ุชูุณูุน ุงููุฑุงุฏูุงุช ูุชุบุทูุฉ ูุฒูุฏ ูู ุงูุญุงูุงุช ุงููุงุฏูุฉ ูู ูููุงุช ุฃู ุฎุฑุงุฆุท ูุฎุชููุฉ
    serial_cands = [
        'ูุณูุณู', 'ูุณูุณู ุงููุงูููุฉ', 'Serial', 'POS Serial', 'POS', 'ุฑูู ุงููุงูููุฉ',
        'serial', 'pos serial', 'sn', 'POS_SN'
    ]
    code_cands = [
        'ุฑูู ุงูุนููู', 'Customer Code', 'Customer_ID', 'client_code', 'trader_id',
        'code', 'ID', 'id', 'Bakery ID', 'ุฑูู ุงููุฎุจุฒ', 'ุฑูู ุงูุชุงุฌุฑ'
    ]
    name_cands = [
        'ุงุณู ุงูุนููู', 'ุงุณู ุงููุฎุจุฒ', 'ุงุณู ุงูุชุงุฌุฑ', 'Customer Name', 'Trader Name', 'Bakery Name', 'trader_name',
        'name', 'customer_name', 'trader_name_ar'
    ]
    # ุฅุถุงูุฉ ูุฑุงุฏูุงุช ูุนููุฏ ุงูููุน ูุถูุงู ุงูููุชุฑุฉ ุงูุตุญูุญุฉ ุญุณุจ ุงููุณู
    type_cands = ['ุงูููุน', 'ููุน', 'Type', 'type', 'Category', 'category']

    def pick(cands):
        for c in cands:
            if c in cols:
                return c
        return None

    ren = {}
    # ูุญุงููุฉ ุงูุชูุงุท ุนููุฏ ุงูุชุงุฑูุฎ ุจุงูุงุดุชูุงู ุฅุฐุง ูู ููุนุซุฑ ุนููู ุตุฑุงุญุฉู
    def fuzzy_pick_date(cols_list: list[str]) -> str | None:
        lowered = [str(c).lower() for c in cols_list]
        for i, lc in enumerate(lowered):
            if ('ุชุงุฑูุฎ' in lc) or ('ุงุฑูุฎ' in lc) or ('date' in lc) or ('visit' in lc and 'date' in lc):
                return cols_list[i]
        return None

    # ุฅูุชูุงุท ูุฑู ูุนููุฏ ุงูุฑูู (code/id) ุฅุฐุง ูู ููุนุซุฑ ุนููู ูุจุงุดุฑุฉู
    def fuzzy_pick_code(cols_list: list[str]) -> str | None:
        lowered = [str(c).lower() for c in cols_list]
        for i, lc in enumerate(lowered):
            if ('code' in lc) or (lc == 'id') or ('trader' in lc and 'id' in lc) or ('bakery' in lc and 'id' in lc) or ('ุฑูู' in lc and 'ูููู' not in lc and 'ูุณูุณู' not in lc):
                return cols_list[i]
        return None

    # ุฅูุชูุงุท ูุฑู ูุนููุฏ ุงููุณูุณู ุฅุฐุง ูุงูุช ุงูุฃุณูุงุก ุจุญุฑูู ุตุบูุฑุฉ ุฃู ูุฎุชุตุฑุฉ
    def fuzzy_pick_serial(cols_list: list[str]) -> str | None:
        lowered = [str(c).lower() for c in cols_list]
        for i, lc in enumerate(lowered):
            if ('serial' in lc) or ('sn' in lc) or ('pos' in lc and 'serial' in lc) or ('ูุณูุณู' in lc):
                return cols_list[i]
        return None

    d = pick(date_cands) or fuzzy_pick_date(cols)
    s = pick(serial_cands) or fuzzy_pick_serial(cols)
    c = pick(code_cands) or fuzzy_pick_code(cols)
    n = pick(name_cands)
    tcol = pick(type_cands)
    if d: ren[d] = 'ุงูุชุงุฑูุฎ'
    if s: ren[s] = 'ูุณูุณู'
    if c: ren[c] = 'ุฑูู ุงูุนููู'
    if n: ren[n] = 'ุงุณู ุงูุนููู'
    if tcol: ren[tcol] = 'ุงูููุน'
    if ren:
        out = out.rename(columns=ren)

    # ุฅุฐุง ูู ูุชู ุงูุชุนุฑู ุนูู ุงูุชุงุฑูุฎุ ุฌุฑูุจ ุงูุชุดุงูู ุญุณุจ ุงูููุน: ุฅุฐุง ููุฌุฏ ุนููุฏ datetime ุฃู ุนููุฏ ููููู ุชุญููู ุบุงูุจ ูููู ูุชุงุฑูุฎ
    if 'ุงูุชุงุฑูุฎ' not in out.columns:
        try:
            # ูุฑุดุญ: ุนููุฏ ููุนู datetime ูุจุงุดุฑุฉู
            for c in out.columns:
                if np.issubdtype(out[c].dtype, np.datetime64):
                    out = out.rename(columns={c: 'ุงูุชุงุฑูุฎ'})
                    break
            # ุฅู ูู ูููุชูุทุ ุฌุฑูุจ ุฃูุถู ุนููุฏ ูููู ุชุญูููู
            if 'ุงูุชุงุฑูุฎ' not in out.columns:
                best_col = None; best_score = -1
                for c in out.columns:
                    ser = out[c]
                    # ุชูููู ูุงุจููุฉ ุชุญููู ุงููุตูุต/ุงูุฃุฑูุงู ุฅูู ุชุงุฑูุฎ
                    try:
                        # ุชุฑุฌูุฉ ุฃุฑูุงู ุนุฑุจูุฉ ูุชูููู
                        tmp = ser.astype(str).str.translate(_AR_DIGITS).str.strip()
                        dt1 = pd.to_datetime(tmp, errors='coerce', dayfirst=True)
                        dt2 = pd.to_datetime(tmp, errors='coerce', dayfirst=False)
                        score = max(dt1.notna().sum(), dt2.notna().sum())
                    except Exception:
                        score = -1
                    # ุฏุนู ุงูุชูุงุฑูุฎ ุงูุฑูููุฉ ุนูู ุทุฑููุฉ Excel
                    try:
                        # ุฅุฐุง ูุงู ุงูุนููุฏ ุฑููููุง ูู ูุนุธู ุตูููู
                        num_ratio = pd.to_numeric(ser, errors='coerce').notna().mean()
                        if num_ratio > 0.6:
                            dt_num = pd.to_datetime(pd.to_numeric(ser, errors='coerce'), unit='d', origin='1899-12-30', errors='coerce')
                            score = max(score, dt_num.notna().sum())
                    except Exception:
                        pass
                    if score > best_score:
                        best_score = score; best_col = c
                if best_col and best_score > 0:
                    out = out.rename(columns={best_col: 'ุงูุชุงุฑูุฎ'})
        except Exception:
            pass

    # ุชุญููู ุงูุฃููุงุน ุฅูู ูุต ูุญููู ุงููุทุงุจูุฉ ูุชุฌููุจ ุนุฏู ุงูุชุทุงุจู ุจุณุจุจ ุงูุฃููุงุน
    for k in ['ุฑูู ุงูุนููู', 'ุงุณู ุงูุนููู', 'ูุณูุณู']:
        if k in out.columns:
            try:
                out[k] = out[k].astype(str).str.strip()
            except Exception:
                pass
    return _drop_empty_columns(out)


def _inquiry_search(category: str, search_type: str, query: str, visit_period: str = 'recent_program') -> dict:
    """ุชูููุฐ ุงูุจุญุซ ุงูุณุฑูุน ุฏุงุฎู ุจูุงูุงุช ุงูุชูุฑูุฑ ุงููุฎุฒูุฉ ูุฅุนุงุฏุฉ ููููุฉ ุงููุชุงุฆุฌ.
    ุชู ุชุญุณูู ุงูุฃุฏุงุก ุจุงุณุชุฎุฏุงู ููุงุฑุณ ูุงุด ูู ุงูุฐุงูุฑุฉ ููุณุงุฑ ุณุฑูุนุ ูุน ูุณุงุฑ ุงุญุชูุงุทู ููุชุตููุฉ ุงูุชูููุฏูุฉ ุนูุฏ ุงูุญุงุฌุฉ.
    """
    
    # ุงุณุชุฎุฏู ุงููุงุด ุงูููููุฑุณ ุจุฏูุงู ูู ุฅุนุงุฏุฉ ุจูุงุก ุงููุงุจูุฌ ูู ูู ุทูุจ
    cached = _get_inquiry_cache(category)
    # ุชุฌูุจ ุชูููู ุงูุญูููุฉ ุงูุบุงูุถ ูู DataFrame ุนูุฏ ุงุณุชุฎุฏุงู "or"
    mapped_df = cached.get('df') if (cached.get('df') is not None) else pd.DataFrame()
    all_cols = cached.get('cols') or []
    indexes = cached.get('indexes') or {}

    if mapped_df.empty:
        return {'success': False, 'message': f'ูุง ุชูุฌุฏ ุจูุงูุงุช ูุณุชูุฑุฏุฉ ููุณู {CATEGORIES.get(category, category)}.', 'items': []}
    
    # 1. ุชุญุฏูุฏ ุฃุนูุฏุฉ ุงูุจุญุซ ุจูุงุกู ุนูู ุงูููุน (Search_Type)
    target_cols = []
    
    if search_type == 'code':
        # ุงูุฃููููุฉ ููุจุญุซ ุจุฑูู ุงูุนููู ุซู ุงููุฎุจุฒ ุซู ุงูุชุงุฌุฑ
        target_cols_priority = ['ุฑูู ุงูุนููู', 'ุฑูู ุงููุฎุจุฒ', 'ุฑูู ุงูุชุงุฌุฑ', 'ุฑูู ุงููุงูููุฉ']
        for col in target_cols_priority:
            if col in all_cols:
                target_cols.append(col)
        if not target_cols:
             target_cols = [c for c in all_cols if 'ุฑูู' in c and 'ูุณูุณู' not in c and 'ูููู' not in c]

    elif search_type == 'serial':
        # ุชูุณูุน ุงููุฑุงุฏูุงุช ูุฃุนูุฏุฉ ุงููุณูุณู ูุชุนูู ุนุจุฑ ุฌููุน ุงูุชุจููุจุงุช
        serial_candidates = [
            'ูุณูุณู ุงููุงูููุฉ', 'ูุณูุณู', 'serial', 'pos serial', 'sn',
            'ุฑูู ุงููุงูููุฉ', 'machine serial'
        ]
        target_cols = [
            c for c in all_cols
            if any(tok in str(c).lower() for tok in serial_candidates)
        ]
        # ูู ุญุงู ุนุฏู ุงูุนุซูุฑุ ุฌุฑูุจ ุฃุนูุฏุฉ ุชุญุชูู "ูุณูุณู" ููููุฉ ุนุฑุจูุฉ ุตุฑูุญุฉ
        if not target_cols:
            target_cols = [c for c in all_cols if 'ูุณูุณู' in str(c)]
    
    elif search_type == 'machine_code':
         # ุฏุนู ุงูุจุญุซ ุจุฑูู/ููุฏ ุงููุงูููุฉ ุนุจุฑ ูุฑุงุฏูุงุช ูุชุนุฏุฏุฉ
         mc_tokens = [
             'ุฑูู ุงููุงูููุฉ', 'ููุฏ ุงููุงูููุฉ',
             'machine code', 'machine id', 'pos id', 'terminal id',
             'ุฑูู ุงูุฌูุงุฒ', 'ููุฏ ุงูุฌูุงุฒ'
         ]
         mc_tokens = [t.lower() for t in mc_tokens]
         target_cols = [c for c in all_cols if any(tok in str(c).lower() for tok in mc_tokens)]
         # fallback ุฐูู: ุฃู ุนููุฏ ูุญุชูู "ุฑูู" ูุน "ูุงู" ุฃู ูุญุชูู "code" ู"machine"
         if not target_cols:
             target_cols = [c for c in all_cols if ('ุฑูู' in str(c) and 'ูุงู' in str(c))]
         if not target_cols:
             target_cols = [c for c in all_cols if ('code' in str(c).lower() and 'machine' in str(c).lower())]
        
    elif search_type == 'name':
         # ุงูุฃููููุฉ ููุจุญุซ ุจุงุณู ุงูุนููู ุซู ุงููุฎุจุฒ ุซู ุงูุชุงุฌุฑ
         name_keys_priority = ['ุงุณู ุงูุนููู', 'ุงุณู ุงููุฎุจุฒ', 'ุงุณู ุงูุชุงุฌุฑ']
         target_cols = [c for c in name_keys_priority if c in all_cols]
         if not target_cols:
              target_cols = [c for c in all_cols if 'ุงุณู' in c] # Fallback to any 'ุงุณู' column
        
    # ุฅุฒุงูุฉ ุงูุฃุนูุฏุฉ ุบูุฑ ุงูููุฌูุฏุฉ ูุนูุงู
    target_cols = [col for col in target_cols if col in all_cols]
    # ุฅุฐุง ุชุนุฐุฑ ุชุญุฏูุฏ ุฃุนูุฏุฉ ูุงุถุญุฉ ููุจุญุซุ ูุง ููููู ุงูุนูููุฉ
    # ุจู ูุณุชุฎุฏู ูุณุงุฑ ุชุตููุฉ ุดุงูู ุนุจุฑ ุฌููุน ุงูุฃุนูุฏุฉ ูุญู ุงุญุชูุงุทู
    use_comprehensive_search = (len(target_cols) == 0)

    # 2. ุชูููุฐ ุงูุจุญุซ (ูุณุงุฑ ุณุฑูุน ุนุจุฑ ุงูููุงุฑุณ + ูุณุงุฑ ุงุญุชูุงุทู ุจุงูุชุตููุฉ)
    q_norm = _norm_key_text(query).lower()
    hit_indexes: set[int] = set()
    try:
        print(f"[inquiry_debug] start search_type={search_type} q_norm={q_norm} target_cols={target_cols} use_comp={use_comprehensive_search}")
    except Exception:
        pass

    if q_norm:
            if search_type == 'code':
                # ูุทุงุจูุฉ ุฏูููุฉ ุฃููุงู
                exact_hits = indexes.get('code', {}).get(q_norm, [])
                if exact_hits:
                    hit_indexes.update(exact_hits)
                    try:
                        print(f"[inquiry_debug] path=index_exact_code q={q_norm} hits={len(exact_hits)}")
                    except Exception:
                        pass
            elif search_type == 'serial':
                # ุชูุถูู ุงููุทุงุจูุฉ ุงูุชุงูุฉุ ุงุณุชุฎุฏุงู ุงูุจุงุฏุฆุงุช ููุท ุฅุฐุง ูู ุชูุฌุฏ ูุทุงุจูุฉ ุชุงูุฉ
                exact_hits = indexes.get('serial', {}).get(q_norm, [])
                if exact_hits:
                    hit_indexes.update(exact_hits)
                    try:
                        print(f"[inquiry_debug] path=index_exact_serial q={q_norm} hits={len(exact_hits)}")
                    except Exception:
                        pass
                else:
                    if len(q_norm) >= 5:
                        pref5 = indexes.get('serial_prefix5', {}).get(q_norm[:5], [])
                        hit_indexes.update(pref5)
                        try:
                            print(f"[inquiry_debug] path=index_prefix_serial_5 q={q_norm[:5]} hits={len(pref5)}")
                        except Exception:
                            pass
                    if len(q_norm) >= 3:
                        pref3 = indexes.get('serial_prefix3', {}).get(q_norm[:3], [])
                        hit_indexes.update(pref3)
                        try:
                            print(f"[inquiry_debug] path=index_prefix_serial_3 q={q_norm[:3]} hits={len(pref3)}")
                        except Exception:
                            pass
            elif search_type == 'machine_code':
                # ุชูุถูู ุงููุทุงุจูุฉ ุงูุชุงูุฉุ ุงุณุชุฎุฏุงู ุงูุจุงุฏุฆุงุช ููุท ุฅุฐุง ุบุงุจุช ุงููุทุงุจูุฉ ุงูุชุงูุฉ
                exact_hits = indexes.get('machine_code', {}).get(q_norm, [])
                if exact_hits:
                    hit_indexes.update(exact_hits)
                    try:
                        print(f"[inquiry_debug] path=index_exact_machine_code q={q_norm} hits={len(exact_hits)}")
                    except Exception:
                        pass
                else:
                    if len(q_norm) >= 5:
                        pref5 = indexes.get('machine_code_prefix5', {}).get(q_norm[:5], [])
                        hit_indexes.update(pref5)
                        try:
                            print(f"[inquiry_debug] path=index_prefix_machine_code_5 q={q_norm[:5]} hits={len(pref5)}")
                        except Exception:
                            pass
                    if len(q_norm) >= 3:
                        pref3 = indexes.get('machine_code_prefix3', {}).get(q_norm[:3], [])
                        hit_indexes.update(pref3)
                        try:
                            print(f"[inquiry_debug] path=index_prefix_machine_code_3 q={q_norm[:3]} hits={len(pref3)}")
                        except Exception:
                            pass
            elif search_type == 'name':
                # ุงูุงุณู ุงููุงูู ุฃููุงู
                name_hits = indexes.get('name', {}).get(q_norm, [])
                hit_indexes.update(name_hits)
                try:
                    print(f"[inquiry_debug] path=index_exact_name q={q_norm} hits={len(name_hits)}")
                except Exception:
                    pass
                # ูููุงุช ุงูุงุณู (ูุทุงุจูุฉ ูุงููุฉ)
                for tok in [t for t in q_norm.split(' ') if t]:
                    tok_hits = indexes.get('name_token', {}).get(tok, [])
                    hit_indexes.update(tok_hits)
                    # ุจุงุฏุฆุฉ ูููููุงุช
                    if len(tok) >= 5:
                        pref5 = indexes.get('name_token_prefix5', {}).get(tok[:5], [])
                        hit_indexes.update(pref5)
                    if len(tok) >= 3:
                        pref3 = indexes.get('name_token_prefix3', {}).get(tok[:3], [])
                        hit_indexes.update(pref3)
    
    
    filtered_df = None
    # ุนูุฏ ุงูุจุญุซ ุจููุน 'code' ูุณุชุฎุฏู ูุทุงุจูุฉ contains ุนุจุฑ ุงูุฃุนูุฏุฉ ุงููุญุฏุฏุฉ
    if search_type == 'code' and q_norm:
        try:
            # ุงุณุชุฎุฏู ุฏุงูุฉ ุงูุชุตููุฉ ุงูููุญุฏุฉ ููุจุญุซ ุงูุฌุฒุฆู ุฏุงุฎู ุงูุฃุนูุฏุฉ ุงููุณุชูุฏูุฉ
            filtered_df = _filter_dataframe(mapped_df, query, search_cols=(target_cols or []))
            try:
                print(f"[inquiry_debug] path=code_contains_target_cols q={query} cols={target_cols} count={len(filtered_df) if filtered_df is not None else 0}")
            except Exception:
                pass
        except Exception:
            filtered_df = mapped_df.iloc[0:0]
    if (filtered_df is None) and hit_indexes:
        try:
            filtered_df = mapped_df.loc[sorted(hit_indexes)]
        except Exception:
            # ูู ุญุงู ูุดู ุงูุชูุทูุน ุจุงููุคุดุฑุงุชุ ูุณุชุฎุฏู ุงูุฅุณูุงุท ุนุจุฑ mask
            mask = mapped_df.index.isin(list(hit_indexes))
            filtered_df = mapped_df[mask]
        
    elif filtered_df is None:
        # ุชุตููุฉ ุฐููุฉ ุนุจุฑ ุงูุฃุนูุฏุฉ ุงููุณุชูุฏูุฉุ ูุฅุฐุง ูู ุชูุญุฏุฏ ุฃุนูุฏุฉุ ุงุณุชุฎุฏู ุงูุจุญุซ ุงูุดุงูู ุนุจุฑ ูู ุงูุฃุนูุฏุฉ
        filtered_df = _filter_dataframe(mapped_df, query, search_cols=(None if use_comprehensive_search else target_cols))
        try:
            path = 'comprehensive_contains' if use_comprehensive_search else 'target_cols_contains'
            print(f"[inquiry_debug] path={path} q={query} cols={None if use_comprehensive_search else target_cols} count={len(filtered_df)}")
        except Exception:
            pass

    filtered_df = filtered_df.copy()

    if filtered_df.empty:
        try:
            print(f"[inquiry_debug] empty_result q={query} search_type={search_type}")
        except Exception:
            pass
        return {'success': False, 'message': f'ูู ูุชู ุงูุนุซูุฑ ุนูู ูุชุงุฆุฌ ููุจุญุซ ุนู "{query}".', 'items': []}

    
    filtered_df = _drop_empty_columns(filtered_df) 

    # 4. ุงูุชุฌููุน ุงูุฃุตูู ููุจูุงูุงุช
    grouped_nested_results = _group_search_results(filtered_df, category)
    
    total_found_entities = len(grouped_nested_results)
    
    if not grouped_nested_results:
        try:
            print(f"[inquiry_debug] grouped_empty q={query} filtered_count={len(filtered_df)}")
        except Exception:
            pass
        return {'success': False, 'message': 'ุชู ุงูุนุซูุฑ ุนูู ุณุฌูุงุชุ ููู ูู ูุชู ุชุฌููุนูุง ูู ููุงู ุตุงูุญ (ุจุฑุฌุงุก ุงูุชุญูู ูู ุฃุนูุฏุฉ ุฑูู ุงูุนููู/ุงุณู ุงูุนููู).', 'items': []}

    # 5. ุฅุนุงุฏุฉ ููููุฉ ุงููุชูุฌุฉ ูููุงุฌูุฉ (ูุณุชุฎุฏู ุงููุชูุฌุฉ ุงูุฃููู ููุท)
    first_group = grouped_nested_results[0]
    common_data = first_group['common_data']
    
    # ุชุญุฏูุฏ ููุงุชูุญ ุงูููุงู
    group_keys = first_group.get('group_keys', ['ุฑูู ุงูุนููู', 'ุงุณู ุงูุนููู'])
    customer_code = common_data.get(group_keys[0], '-')
    customer_name = common_data.get(group_keys[1], '-')

    # ๐ก 6. ุงุณุชุฎุฑุงุฌ ุงูุญููู ุงูุฏููุงููููุฉ (ูู ููู all data.xlsx)
    # ุฃุณูุงุก ุงูุฃุนูุฏุฉ ุงููุชููุนุฉ ูู ููู all data.xlsx
    DYNAMIC_FIELDS_COLS = {
        'ูุงูููุฉ ูุฑุน': 'ูุฏูุฉ ูุงูููุฉ ูุฑุน',
        'ูุงูููู ูุฑุน': 'ูุฏูุฉ ูุงูููุฉ ูุฑุน',
        'ุฅุฌูุงูู ุงูุญูุงูุงุช': 'ุฅุฌูุงูู ุงูุญูุงูุงุช',
        'ุงูุญูุงูู': 'ุฅุฌูุงูู ุงูุญูุงูุงุช',
        'ุงุฎุฑ ูุถุน ูููุงูููุฉ': 'ุงุฎุฑ ูุถุน ูููุงูููุฉ',
        'ุงูุญุงูู': 'ุงุฎุฑ ูุถุน ูููุงูููุฉ',
        'ุงุฎุฑ ุชุงุฑูุฎ ุณูุฑ ูููุงูููุฉ': 'ุงุฎุฑ ุชุงุฑูุฎ ุณูุฑ ูููุงูููุฉ',
        'ุชุงุฑูุฎ ุงูุณูุฑ ููุตูุงูู': 'ุงุฎุฑ ุชุงุฑูุฎ ุณูุฑ ูููุงูููุฉ',
        'ุชุงุฑูุฎ ุงูุณูุฑ ููุตูุงูุฉ': 'ุงุฎุฑ ุชุงุฑูุฎ ุณูุฑ ูููุงูููุฉ',
    }
    
    dynamic_fields = {}
    
    # ุงุณุชุฎุฑุงุฌ ุงูุญููู ุงูุฏููุงููููุฉ
    for col_in_data, key_for_display in DYNAMIC_FIELDS_COLS.items():
        dynamic_fields[key_for_display] = common_data.get(col_in_data, '-')
        # ุฅุฒุงูุฉ ุงูุนููุฏ ูู ุงูุจูุงูุงุช ุงููุดุชุฑูุฉ ุจุนุฏ ุงุณุชุฎูุงุตู
        if col_in_data in common_data:
             del common_data[col_in_data] 

    # ุชุญููู ุงููุงูููุงุช ุงูุฃุณุงุณูุฉ ูู ุฎุฏูุงุช ุงูุชุฌุงุฑ (trader_primary) ููุทุงุจูุฉ ุงูููุฏ/ุงูุงุณู
    def _load_primary_df() -> pd.DataFrame:
        try:
            row = _load_state("trader_primary")
            if not row or not row.data_json:
                return pd.DataFrame()
            dfp = _json_to_df(row.data_json)
            map_row = _load_state("trader_primary:__mapping__")
            mapping = json.loads(map_row.mapping_json) if (map_row and map_row.mapping_json) else {}
            dfp = _apply_mapping(dfp, mapping)
            return _drop_empty_columns(dfp)
        except Exception:
            return pd.DataFrame()

    def _detect_primary_key_cols(cols: list[str]) -> tuple[str|None, str|None]:
        """ุชุญุฏูุฏ ุฃุนูุฏุฉ ุงูููุฏ ูุงูุงุณู ุจุฐูุงุก ูู ุฃุณูุงุก ุงูุฃุนูุฏุฉ ุงููุนููุฉ.
        ูุนุชูุฏ ุนูู ูุฌููุนุฉ ูุงุณุนุฉ ูู ุงููุฑุงุฏูุงุช ุงูุนุฑุจูุฉ ูุงูุฅูุฌููุฒูุฉ ููุณุชุฎุฏู ูุทุงุจูุฉ ุชุญุชูู ุนูู ูููุงุช ุฑุฆูุณูุฉ.
        """
        if not cols:
            return None, None

        all_cols_norm = [str(c).strip() for c in cols]

        # ููุฑุฏุงุช ุฃุณุงุณูุฉ
        code_tokens = ['ุฑูู', 'ุงูููุฏ', 'id', 'code']
        name_tokens = ['ุงุณู', 'name']
        entity_tokens = ['ุนููู', 'ุชุงุฌุฑ', 'ูุฎุจุฒ', 'ูุฑุน', 'customer', 'trader', 'bakery', 'branch']

        # ูุฑุดูุญุงุช ูุฑุฌูุญุฉ ุตุฑูุญุฉ ูุงููุฉ ุฃููุงู
        explicit_code_candidates = [
            'ุฑูู ุงูุนููู','Customer Code','Customer_ID','Customer ID','ุฑูู ุงูุชุงุฌุฑ','trader_id','ุฑูู ุงููุฎุจุฒ','Bakery ID'
        ]
        explicit_name_candidates = [
            'ุงุณู ุงูุนููู','ุงุณู ุงููุฎุจุฒ','ุงุณู ุงูุชุงุฌุฑ','Customer Name','Trader Name','trader_name','Bakery Name'
        ]

        def pick_explicit(cands):
            for c in cands:
                if c in all_cols_norm:
                    return c
            return None

        code_col = pick_explicit(explicit_code_candidates)
        name_col = pick_explicit(explicit_name_candidates)

        # ุฅุฐุง ูู ูููุชุดู ุตุฑุงุญุฉูุ ุงุณุชุฎุฏู ูุทุงุจูุฉ ุชุนุชูุฏ ุนูู ุงุญุชูุงุก ุงููููุงุช ุงูุฑุฆูุณูุฉ
        def contains_any(haystack: str, needles: list[str]) -> bool:
            h = haystack.lower()
            return any(n in h for n in needles)

        if code_col is None:
            # ุงุจุญุซ ุนู ุนููุฏ ูุญุชูู ูููุงุช (ุฑูู|ุงูููุฏ|id|code) ูุน ููุงู (ุนููู|ูุฎุจุฒ|ุชุงุฌุฑ|ูุฑุน)
            ranked = []
            for c in all_cols_norm:
                score = 0
                if contains_any(c, code_tokens):
                    score += 2
                if contains_any(c, entity_tokens):
                    score += 1
                if score > 0:
                    ranked.append((score, c))
            if ranked:
                ranked.sort(key=lambda x: (-x[0], len(x[1])))
                code_col = ranked[0][1]

        if name_col is None:
            ranked = []
            for c in all_cols_norm:
                score = 0
                if contains_any(c, name_tokens):
                    score += 2
                if contains_any(c, entity_tokens):
                    score += 1
                if score > 0:
                    ranked.append((score, c))
            if ranked:
                ranked.sort(key=lambda x: (-x[0], len(x[1])))
                name_col = ranked[0][1]

        # Fallback ุฃุฎูุฑ: ุฃู ุนููุฏ ูุญุชูู "ุฑูู" ููููุฏ ู"ุงุณู" ููุงุณู
        if code_col is None:
            for c in all_cols_norm:
                cl = c.lower()
                if ('ุฑูู' in cl) or ('ุงูููุฏ' in cl) or ('id' in cl) or ('code' in cl):
                    code_col = c
                    break
        if name_col is None:
            for c in all_cols_norm:
                cl = c.lower()
                if ('ุงุณู' in cl) or ('name' in cl):
                    name_col = c
                    break

        return code_col, name_col

    PRIMARY_FIELD_CANDS = {
        'ูุฏูุฉ ูุงูููุฉ ูุฑุน': [
            'ูุฏูุฉ ูุงูููุฉ ูุฑุน', 'ูุฏูู ูุงูููุฉ ูุฑุน', 'ูุงูููุฉ ูุฑุน', 'ูุงูููู ูุฑุน', 'Has Branch Machine', 'Branch Machine', 'ููุฌุฏ ูุงูููุฉ ูุฑุน'
        ],
        'ุฅุฌูุงูู ุงูุญูุงูุงุช': [
            'ุฅุฌูุงูู ุงูุญูุงูุงุช', 'ุงุฌูุงูู ุงูุญูุงูุงุช', 'ุงูุญูุงูู', 'ุงูุญูุงูุงุช', 'Total Transfers', 'Total Transfer', 'Transfers', 'Transfer'
        ],
        'ุงุฎุฑ ูุถุน ูููุงูููุฉ': [
            'ุงุฎุฑ ูุถุน ูููุงูููุฉ', 'ุขุฎุฑ ูุถุน ูููุงูููุฉ', 'ุงุฎุฑ ูุถุน', 'ุขุฎุฑ ูุถุน', 'ุงูุญุงูู', 'ุญุงูุฉ ุงููุงูููุฉ', 'Last Status', 'Status'
        ],
        'ุงุฎุฑ ุชุงุฑูุฎ ุณูุฑ ูููุงูููุฉ': [
            'ุงุฎุฑ ุชุงุฑูุฎ ุณูุฑ ูููุงูููุฉ', 'ุขุฎุฑ ุชุงุฑูุฎ ุณูุฑ ูููุงูููุฉ', 'ุงุฎุฑ ุชุงุฑูุฎ ุณูุฑ', 'ุขุฎุฑ ุชุงุฑูุฎ ุณูุฑ', 'ุชุงุฑูุฎ ุงูุณูุฑ ููุตูุงูู', 'ุชุงุฑูุฎ ุงูุณูุฑ ููุตูุงูุฉ', 'Last Travel Date', 'Travel Date for Maintenance', 'Travel Date'
        ],
        'ุงููุงุฆู ุจุชุณููู ูุงูููุฉ ุงููุฑุน': [
            'ุงููุงุฆู ุจุชุณููู ูุงูููุฉ ุงููุฑุน', 'ุงููุงุฆู ุจุงูุชุณููู', 'ูุณุคูู ุงูุชุณููู', 'ูุณุฆูู ุงูุชุณููู', 'Responsible for Branch Machine Delivery'
        ],
    }

    primary_debug = {}
    primary_record = {}
    primary_match_mode = 'none'
    _primary_df = _load_primary_df()
    if not _primary_df.empty:
        code_col, name_col = _detect_primary_key_cols(list(_primary_df.columns))
        code_norm = _norm_key_text(str(customer_code)) if _textify(customer_code) != '' else None
        name_norm = _norm_key_text(str(customer_name)) if _textify(customer_name) != '' else None
        mask = None
        if code_col and code_norm is not None:
            mask = (_primary_df[code_col].astype(str).apply(_norm_key_text) == code_norm)
        if name_col and name_norm is not None:
            nm = (_primary_df[name_col].astype(str).apply(_norm_key_text) == name_norm)
            mask = (mask & nm) if mask is not None else nm
        filtered_primary = _primary_df[mask] if mask is not None else pd.DataFrame()
        primary_debug = {
            'source_cols': list(_primary_df.columns),
            'match_cols': {'code': code_col, 'name': name_col},
            'match_rows': int(filtered_primary.shape[0]) if not filtered_primary.empty else 0,
        }
        if not filtered_primary.empty:
            primary_match_mode = 'intersection'
        # ุฅู ูู ูุฌุฏ ุตููุง ุจุงุณุชุฎุฏุงู ุงูุชูุงุทุนุ ุฌุฑูุจ ุงูุงุชุญุงุฏ (ูุทุงุจูุฉ ุจุงูุฑูู ุฃู ุงูุงุณู)
        if (filtered_primary.empty) and (code_col or name_col):
            union_mask = None
            if code_col and code_norm is not None:
                union_mask = (_primary_df[code_col].astype(str).apply(_norm_key_text) == code_norm)
            if name_col and name_norm is not None:
                nm2 = (_primary_df[name_col].astype(str).apply(_norm_key_text) == name_norm)
                union_mask = (union_mask | nm2) if union_mask is not None else nm2
            if union_mask is not None:
                filtered_primary = _primary_df[union_mask]
                primary_debug['match_rows_union'] = int(filtered_primary.shape[0])
                primary_debug['used_union'] = True
                if not filtered_primary.empty:
                    primary_match_mode = 'union'
        if not filtered_primary.empty:
            # ุงุฎุชูุงุฑ ุงูุตู ุงูุฃูุถู ุฅุฐุง ุชุนุฏุฏุช ุงูุตููู ูู ุงูุงุชุญุงุฏ
            try:
                best_idx = 0
                if filtered_primary.shape[0] > 1:
                    # ุญุงูู ูุทุงุจูุฉ ุงูุงุณู ุจุฏูุฉ ุฅุฐุง ุชููุฑ
                    if name_col and name_norm is not None and name_col in filtered_primary.columns:
                        exact_name = filtered_primary[name_col].astype(str).apply(_norm_key_text) == name_norm
                        match_idxs = list(filtered_primary[exact_name].index)
                        if match_idxs:
                            best_idx = match_idxs[0]
                    # ุฃู ูุทุงุจูุฉ ุงูููุฏ ุจุฏูุฉ ุฅุฐุง ุชููุฑ
                    elif code_col and code_norm is not None and code_col in filtered_primary.columns:
                        exact_code = filtered_primary[code_col].astype(str).apply(_norm_key_text) == code_norm
                        match_idxs = list(filtered_primary[exact_code].index)
                        if match_idxs:
                            best_idx = match_idxs[0]
                first_row = filtered_primary.loc[best_idx].to_dict()
            except Exception:
                first_row = filtered_primary.iloc[0].to_dict()
            # ุญูุธ ุงูุตู ุงููุงูู ูุงุณุชุฎุฏุงูู ูู ุงููุงุฌูุฉ
            try:
                primary_record = {k: ('' if _textify(v) == '' else v) for k, v in first_row.items()}
            except Exception:
                primary_record = first_row
            # Override ููู ุงูุญููู ุงูุฏููุงููููุฉ ูู ูุตุฏุฑ ุงููุงูููุงุช ุงูุฃุณุงุณูุฉ ุฅุฐุง ุชููุฑุช
            for display_key, cand_list in PRIMARY_FIELD_CANDS.items():
                val = None
                for cand in cand_list:
                    if cand in first_row and _textify(first_row.get(cand)) != '':
                        val = first_row.get(cand)
                        break
                if val is not None:
                    dynamic_fields[display_key] = val

    # ุจูุงุก ุฌุฒุก ูุงูููุงุช ุงููุฑุน ูุงูุญูุงูุงุช ูู ุตู All Data ุงููุทุงุจู ุชูุงุทุนุงู ููุท
    branch_section = {
        'ูุงูููู ูุฑุน': '-',
        'ุงูุญูุงูู': '-',
        'ุงูุญุงูู': '-',
        'ุชุงุฑูุฎ ุงูุณูุฑ ููุตูุงูู': '-',
        'ุงููุงุฆู ุจุชุณููู ูุงูููุฉ ุงููุฑุน': '-',
    }
    try:
        if primary_record:
            # ุงุณุชุฎุฏู ุงููุฑุงุฏูุงุช ููุนุซูุฑ ุนูู ุงูููู ุงููุทููุจุฉ ูู ุงูุตู (ุณูุงุก ุชูุงุทุน ุฃู ุงุชุญุงุฏ)
            field_synonyms = {
                'ูุงูููู ูุฑุน': ['ูุงูููู ูุฑุน', 'ูุงูููุฉ ูุฑุน', 'ูุฏูุฉ ูุงูููุฉ ูุฑุน', 'ูุฏูู ูุงูููุฉ ูุฑุน', 'Has Branch Machine', 'Branch Machine', 'ููุฌุฏ ูุงูููุฉ ูุฑุน'],
                'ุงูุญูุงูู': ['ุงูุญูุงูู', 'ุงูุญูุงูุงุช', 'ุฅุฌูุงูู ุงูุญูุงูุงุช', 'Total Transfers', 'Transfers'],
                'ุงูุญุงูู': ['ุงูุญุงูู', 'ุญุงูุฉ ุงููุงูููุฉ', 'ุงุฎุฑ ูุถุน ูููุงูููุฉ', 'ุขุฎุฑ ูุถุน ูููุงูููุฉ', 'Last Status', 'Status'],
                'ุชุงุฑูุฎ ุงูุณูุฑ ููุตูุงูู': ['ุชุงุฑูุฎ ุงูุณูุฑ ููุตูุงูู', 'ุชุงุฑูุฎ ุงูุณูุฑ ููุตูุงูุฉ', 'ุงุฎุฑ ุชุงุฑูุฎ ุณูุฑ ูููุงูููุฉ', 'ุขุฎุฑ ุชุงุฑูุฎ ุณูุฑ ูููุงูููุฉ', 'ุขุฎุฑ ุชุงุฑูุฎ ุณูุฑ', 'Last Travel Date'],
                'ุงููุงุฆู ุจุชุณููู ูุงูููุฉ ุงููุฑุน': ['ุงููุงุฆู ุจุชุณููู ูุงูููุฉ ุงููุฑุน', 'ุงููุงุฆู ุจุงูุชุณููู', 'ูุณุคูู ุงูุชุณููู', 'ูุณุฆูู ุงูุชุณููู']
            }
            for display_key, cands in field_synonyms.items():
                for c in cands:
                    if c in primary_record and _textify(primary_record.get(c)) != '':
                        branch_section[display_key] = primary_record.get(c)
                        break
    except Exception:
        pass
    
    # 7. ุชุฌููุฒ ุจูุงูุงุช ุงููุณูุณูุงุช ูุจูุฑูุง ูุงุณุชุฎุฏุงููุง ูู ุชุตููุฉ ุณุฌู ุงูุฒูุงุฑุงุช
    
    # ๐ก ุชู ุฅุฒุงูุฉ ุฏุงูุฉ convert_primary_secondary ูููุทู ุชุทุจูููุงุ ุญูุซ ุฃู ุงูุชุญููู ุฃุตุจุญ ูุชู
    # ูุจุงุดุฑุฉ ุจุนุฏ ุงูุงุณุชูุฑุงุฏ ูู ุฏุงูุฉ _merge_all.
    
    serial_list = []
    # ๐ก ุฎุฑูุทุฉ ูุฃุณูุงุก ุฃุนูุฏุฉ ุงููุณูุณูุงุช
    SERIAL_MAP = {
        'ูุณูุณู ุงููุงูููุฉ': 'ูุณูุณู',
        'ุฑูู ุงููุงูููุฉ': 'ุฑูู ุงููุงูููุฉ',
        'ูุงูููุฉ ุฑุฆูุณูุฉ/ูุฑุนูุฉ': 'ุฑุฆูุณูุฉ/ูุฑุนูุฉ', 
        'ุดุฑูุญุฉ 1': 'ุดุฑูุญุฉ1',
        'ุดุฑูุญุฉ 2': 'ุดุฑูุญุฉ2',
        'ุญุงูุฉ ุงููุงูููุฉ': 'ุญุงูุฉ ุงููุงูููุฉ',
        'ุญุงูุฉ ูุธุงู ุงููุทุญู': 'ุญุงูุฉ ูุธุงู ุงููุทุญู', 
        'SW_AC_SUP': 'SW_AC_SUP', 
        'SW_IC_SUP': 'SW_IC_SUP',
        'SW_OD_SUP': 'SW_OD_SUP',
        'POS_VERSION': 'POS_VERSION', 
        'ุงุณู ุงูุฎุจุฒ': 'ุงุณู ุงูุฎุจุฒ',
        'LOAF_BALANCE1': 'LOAF_BALANCE1',
        'ุณุงุนุฉ ุจุฏุก ุงูุจูุน': 'ุณุงุนุฉ ุจุฏุก ุงูุจูุน',
        'ุณุงุนุฉ ููุงูุฉ ุงูุจูุน': 'ุณุงุนุฉ ููุงูุฉ ุงูุจูุน',
    }
    
    for machine in first_group['machine_details']:
        serial_item = {}
        for original_col, new_key in SERIAL_MAP.items():
            value = machine.get(original_col, '-')
            
            # ๐ก ุจูุง ุฃู ุงูุชุญููู ุชู ุนูุฏ ุงูุงุณุชูุฑุงุฏุ ููุชูู ุจุฃุฎุฐ ุงููููุฉ ูุจุงุดุฑุฉ
            serial_item[new_key] = value
            
        serial_list.append(serial_item)
        
    # 9. ุชุฌููุฒ ุจูุงูุงุช ุงูุนููู ุงูุฃุณุงุณูุฉ ุจุงูุชุฑุชูุจ ุงููุทููุจ
    
    # ๐ก ุงูุชุนุฏูู: ุฎุฑูุทุฉ ุงูุฃุนูุฏุฉ ููุจุญุซ ุนู ุจุฏุงุฆู ูู ุงูุจูุงูุงุช ุงููุณุชูุฑุฏุฉ (ูุฅุตูุงุญ ุฑูู ุงููุญููู ูุงุณู ุงููุณุคู)
    CUSTOMER_FETCH_MAP = {
        'ุฑูู ุงููุญููู': ['ุฑูู ุงููุญููู', 'ุงููุญููู', 'ููุจุงูู', 'ุฑูู ุงููุงุชู ุงููุญููู'],
        'ุงุณู ุงููุณุคู': ['ุงุณู ุงููุณุคู', 'ุงุณู ุงููุณุฆูู', 'ุงููุณุฆูู', 'ุงููุณุคูู', 'ูุณุฆูู'], 
        
        'ุฑูู ูุงุชู': ['ุฑูู ูุงุชู', 'ุชููููู', 'ูุงุชู', 'ุซุงุจุช'],
        'ุงูุฑูู ุงููููู': ['ุงูุฑูู ุงููููู', 'ุจุทุงูุฉ', 'ุฑูู ุจุทุงูุฉ'],
        'ุงูุงุฏุงุฑุฉ': ['ุงูุงุฏุงุฑุฉ', 'ุงููุฏูุฑูุฉ'],
        'ุงูููุชุจ': ['ุงูููุชุจ', 'ุงูุดุนุจุฉ'],
        'ุฑูู ุงูุนููู': ['ุฑูู ุงูุนููู', 'ููุฏ ุงูุนููู'],
        'ุงุณู ุงูุนููู': ['ุงุณู ุงูุนููู', 'ุงุณู ุงููุฎุจุฒ', 'ุงุณู ุงูุชุงุฌุฑ'],
        'ุญุงูุฉ ุงูุนููู': ['ุญุงูุฉ ุงูุนููู', 'ุญุงูุฉ'],
        'ููุงุญุธุงุช': ['ููุงุญุธุงุช', 'Note'],
    }
    
    customer_data = {}
    for display_key in CUSTOMER_DETAIL_COLS:
        value = '-'
        # ูุชู ุงุณุชุฎุฏุงู ููุชุงุญ ุงูุนุฑุถ ุฃููุงูุ ูุฅุฐุง ูู ููู ููุฌูุฏูุงุ ูุชู ุงูุจุญุซ ูู ุงูุจุฏุงุฆู
        potential_keys = CUSTOMER_FETCH_MAP.get(display_key, [display_key])
        
        for p_key in potential_keys:
            # ุงูุชุญูู ูู ูุฌูุฏ ุงูููุชุงุญ ูู ุงูุจูุงูุงุช ุงููุดุชุฑูุฉ ูุฃู ูููุชู ููุณุช ูุงุฑุบุฉ
            if p_key in common_data and _textify(common_data.get(p_key)) != '':
                value = common_data.get(p_key)
                break
                
        customer_data[display_key] = value

    # 10. ุงูุฑุณุงูุฉ ุงูููุงุฆูุฉ
    message = f'ุชู ุงูุนุซูุฑ ุนูู {total_found_entities} ุณุฌู(ุงุช) ููุงู ูุทุงุจู. (ุงูุนููู: {customer_name})'
    if total_found_entities > 1:
        message += ' ููููู ุงูุชููู ุจูู ุงูููุงูุงุช ุจุงุณุชุฎุฏุงู ุฃุฒุฑุงุฑ ุงูุชููู ุฃู ููุชุงุญู ุงูุณูููู (โ ู โ).'
    
    # 9. ุชุญููู ุณุฌู ุงูุฒูุงุฑุงุช ุงููุนูู ููุณูุฉ/ุงูุดูุฑ ุงูุญุงูููู ูู ุฎุฏูุงุช ุงูุชุฌุงุฑ (ุงููุชุฑุฏุฏูู)
    def _load_visit_history_df() -> tuple[pd.DataFrame, dict]:
        global _VISIT_CACHE
        # ููุงุญุธุฉ: ุจูุงูุงุช ุฎุฏูุงุช ุงูุชุฌุงุฑ ุชูุฎุฒู ุนุงูููุงู ุจุฏูู user_id
        # ุณูุฌููุน ุจูุงูุงุช ุงูุณูุฉ ุงูุญุงููุฉ ูู ุฌููุน ุงููุชุฑุงุช ุงููุชุงุญุฉ (label="YYYY" ุฃู "YYYY-MM")
        try:
            now = datetime.now()
            year = now.year
            month = now.month
            month_label = f"{year}-{month:02d}"

            # ุงุณุชุฎุฏุงู ูุงุด ุฎููู ูุชูููู ุงููุฑุงุกุฉ ุงููุชูุฑุฑุฉ ูููุณ ุงููุชุฑุฉ
            try:
                if (_VISIT_CACHE.get('df') is not None) and (time() - (_VISIT_CACHE.get('ts') or 0) < _VISIT_CACHE_TTL_SEC):
                    cached_df = _VISIT_CACHE['df']
                    cached_meta = _VISIT_CACHE['meta']
                    return cached_df.copy(), (dict(cached_meta) if isinstance(cached_meta, dict) else cached_meta)
            except Exception:
                pass

            # ูุฑุถ ุงุณุชุฎุฏุงู ุจูุงูุงุช ุงูุจุฑูุงูุฌ ุงูุญุฏูุซุฉ ุฃููุงู ุฏุงุฆููุง
            try:
                rp_rows = (ReportState.query
                           .filter(ReportState.category == 'trader_frequent:recent_program')
                           .order_by(ReportState.created_at.desc())
                           .all())
                map_row = ReportState.query.filter_by(category="trader_frequent:__mapping__").first()
                mapping = json.loads(map_row.mapping_json) if (map_row and map_row.mapping_json) else {}
                df = pd.DataFrame()
                for r in rp_rows:
                    if not r.data_json:
                        continue
                    d = _json_to_df(r.data_json)
                    if not d.empty:
                        d = _apply_mapping(d, mapping)
                        try:
                            d['_ุงููุชุฑุฉ'] = 'ุงูุจูุงูุงุช ุงูุญุฏูุซุฉ (ุงูุจุฑูุงูุฌ)'
                        except Exception:
                            pass
                        df = pd.concat([df, d], ignore_index=True) if not df.empty else d
                result_df = _drop_empty_columns(df)
                meta = {'month_label': None, 'year_label': None, 'recent_program': True, 'source': 'recent_program'}
                try:
                    _VISIT_CACHE = {'df': result_df.copy(), 'meta': meta.copy(), 'ts': time()}
                except Exception:
                    pass
                return result_df, meta
            except Exception:
                # ุฅุฐุง ูุดูุช ุงููุฑุงุกุฉ ุงูุญุฏูุซุฉุ ูููู ุจููุทู ุงูุดูุฑ/ุงูุณูุฉ
                pass

            # ุงุณุชุนูุงูุงุช ูุณุชูุฏูุฉ ุจุฏู ูุณุญ ุฌููุน ุงููุชุฑุงุช
            exact_month_row = (ReportState.query
                               .filter(ReportState.category == f"trader_frequent:{month_label}")
                               .order_by(ReportState.created_at.desc())
                               .first())
            year_rows = (ReportState.query
                         .filter(ReportState.category == f"trader_frequent:{year}")
                         .order_by(ReportState.created_at.desc())
                         .all())
            months_dash = (ReportState.query
                           .filter(ReportState.category.like(f"trader_frequent:{year}-%"))
                           .order_by(ReportState.created_at.desc())
                           .all())
            months_slash = (ReportState.query
                            .filter(ReportState.category.like(f"trader_frequent:{year}/%"))
                            .order_by(ReportState.created_at.desc())
                            .all())
            months_of_year_rows = months_dash + months_slash
            fallback_month_row = (ReportState.query
                                  .filter(ReportState.category.like("trader_frequent:%-%"))
                                  .order_by(ReportState.created_at.desc())
                                  .first())
            # ุชุนููู ูุฎุทุท ุฅุนุงุฏุฉ ุงูุชุณููุฉ ูุชุฑุชูุจ/ุฅุนุงุฏุฉ ุชุณููุฉ ุงูุฃุนูุฏุฉ (ุฅู ููุฌุฏ)
            map_row = ReportState.query.filter_by(category="trader_frequent:__mapping__").first()
            mapping = json.loads(map_row.mapping_json) if (map_row and map_row.mapping_json) else {}

            # ุงูุชูุงุก ูุชุฑุงุช ุงูุณูุฉ ุงูุญุงููุฉ
            def _label_of(cat: str) -> str:
                try:
                    return cat.split(":", 1)[1]
                except Exception:
                    return ""

            # ุชู ุชูููุฏ year_rows ู months_of_year_rows ู exact/fallback ุนุจุฑ ุงุณุชุนูุงูุงุช ูุณุชูุฏูุฉ ุฃุนูุงู

            # 1) ุชุฌููุน ุงูุณูุฉ: ุฅูุง ูู ููู ุณูุฉ ูุงุญุฏุฉ ุฃู ูู ุดููุฑ ุงูุณูุฉ
            df_year = pd.DataFrame()
            if year_rows:
                for r in year_rows:
                    d = _json_to_df(r.data_json)
                    if not d.empty:
                        d = _apply_mapping(d, mapping)
                        try:
                            d['_ุงููุชุฑุฉ'] = str(year)
                        except Exception:
                            pass
                        df_year = pd.concat([df_year, d], ignore_index=True) if not df_year.empty else d
            elif months_of_year_rows:
                for r in months_of_year_rows:
                    d = _json_to_df(r.data_json)
                    if not d.empty:
                        d = _apply_mapping(d, mapping)
                        try:
                            lab = _label_of(r.category)
                            d['_ุงููุชุฑุฉ'] = lab.replace('/', '-')
                        except Exception:
                            pass
                        df_year = pd.concat([df_year, d], ignore_index=True) if not df_year.empty else d

            # 2) ุชุญุฏูุฏ ููู ุงูุดูุฑ: ูุทุงุจู ููุดูุฑ ุงูุญุงูู ุฃู ุฃุญุฏุซ ููู ุดูุฑู ูุชุงุญ
            selected_month_row = exact_month_row or fallback_month_row
            selected_month_label = None
            df_month = pd.DataFrame()
            if selected_month_row is not None:
                selected_month_label = _label_of(selected_month_row.category).replace('/', '-')
                d = _json_to_df(selected_month_row.data_json)
                if not d.empty:
                    d = _apply_mapping(d, mapping)
                    try:
                        d['_ุงููุชุฑุฉ'] = selected_month_label
                    except Exception:
                        pass
                    df_month = d
            # ุญุณุงุจ ูุฌููุนุฉ ุงูุนุถููุฉ ูู ุจูุงูุงุช ุงูุจุฑูุงูุฌ ุงูุญุฏูุซุฉ (recent_program)
            code_set = set()
            serial_set = set()
            try:
                rp_rows_for_sets = (ReportState.query
                                    .filter(ReportState.category == 'trader_frequent:recent_program')
                                    .order_by(ReportState.created_at.desc())
                                    .all())
                rp_df_sets = pd.DataFrame()
                for rr in rp_rows_for_sets:
                    if not rr.data_json:
                        continue
                    dd = _json_to_df(rr.data_json)
                    if not dd.empty:
                        dd = _apply_mapping(dd, mapping)
                        rp_df_sets = pd.concat([rp_df_sets, dd], ignore_index=True) if not rp_df_sets.empty else dd
                if not rp_df_sets.empty:
                    std_rp = _standardize_visit_df(rp_df_sets)
                    if 'ุฑูู ุงูุนููู' in std_rp.columns:
                        code_set = set(std_rp['ุฑูู ุงูุนููู'].astype(str).apply(_norm_key_text))
                    if 'ูุณูุณู' in std_rp.columns:
                        serial_set = set(std_rp['ูุณูุณู'].astype(str).apply(_norm_key_text))
            except Exception:
                pass

            # ุงุฎุชูุงุฑ ุงููุตุฏุฑ ุชููุงุฆูุงู:
            # - ุฅุฐุง ุชููุฑ ููู ุงูุณูุฉ ุงูุญุงููุฉ: ูุณุชุฎุฏูู ููุท ููุตูู ุชุญุช "ุงูุณูุฉ ุงูุญุงููุฉ"
            # - ูุฅูุง ุฅุฐุง ุชููุฑ ููู ุงูุดูุฑ ุงูุญุงูู/ุฃุญุฏุซ ุดูุฑ: ูุณุชุฎุฏูู ููุท ููุตูู ุชุญุช "ุงูุดูุฑ ุงูุญุงูู"
            # - ูุฅูุง ูุณุชุฎุฏู ุจูุงูุงุช ุงูุจุฑูุงูุฌ ุงูุญุฏูุซุฉ ููุตูููุง ุชุญุช "ุงูุดูุฑ ุงูุญุงูู"

            if not df_year.empty:
                # ููุชุฑุฉ ุงูุณูุฉ ููู ุนุถููุฉ recent_program ุฅู ุชููุฑุช ูุฌููุนุงุช ุนุถููุฉ
                if code_set or serial_set:
                    std_y = _standardize_visit_df(df_year)
                    mask_code = std_y['ุฑูู ุงูุนููู'].astype(str).apply(_norm_key_text).isin(code_set) if ('ุฑูู ุงูุนููู' in std_y.columns and code_set) else None
                    mask_serial = std_y['ูุณูุณู'].astype(str).apply(_norm_key_text).isin(serial_set) if ('ูุณูุณู' in std_y.columns and serial_set) else None
                    if mask_code is not None and mask_serial is not None:
                        year_mask = (mask_code | mask_serial)
                    elif mask_code is not None:
                        year_mask = mask_code
                    elif mask_serial is not None:
                        year_mask = mask_serial
                    else:
                        year_mask = pd.Series([True] * std_y.shape[0])
                    df_year = df_year[year_mask.values]
                result_df = _drop_empty_columns(df_year)
                meta = {'month_label': None, 'year_label': str(year), 'source': 'year'}
                try:
                    _VISIT_CACHE = {'df': result_df.copy(), 'meta': meta.copy(), 'ts': time()}
                except Exception:
                    pass
                return result_df, meta
            if not df_month.empty:
                # ููุชุฑุฉ ุงูุดูุฑ ููู ุนุถููุฉ recent_program ุฅู ุชููุฑุช ูุฌููุนุงุช ุนุถููุฉ
                if code_set or serial_set:
                    std_m = _standardize_visit_df(df_month)
                    mask_code = std_m['ุฑูู ุงูุนููู'].astype(str).apply(_norm_key_text).isin(code_set) if ('ุฑูู ุงูุนููู' in std_m.columns and code_set) else None
                    mask_serial = std_m['ูุณูุณู'].astype(str).apply(_norm_key_text).isin(serial_set) if ('ูุณูุณู' in std_m.columns and serial_set) else None
                    if mask_code is not None and mask_serial is not None:
                        month_mask = (mask_code | mask_serial)
                    elif mask_code is not None:
                        month_mask = mask_code
                    elif mask_serial is not None:
                        month_mask = mask_serial
                    else:
                        month_mask = pd.Series([True] * std_m.shape[0])
                    df_month = df_month[month_mask.values]
                result_df = _drop_empty_columns(df_month)
                meta = {'month_label': selected_month_label, 'year_label': None, 'source': 'month'}
                try:
                    _VISIT_CACHE = {'df': result_df.copy(), 'meta': meta.copy(), 'ts': time()}
                except Exception:
                    pass
                return result_df, meta

            # ุจูุงูุงุช ุงูุจุฑูุงูุฌ ุงูุญุฏูุซุฉ ูุญู ุงุญุชูุงุทู
            try:
                rp_rows = (ReportState.query
                           .filter(ReportState.category == 'trader_frequent:recent_program')
                           .order_by(ReportState.created_at.desc())
                           .all())
                map_row = ReportState.query.filter_by(category="trader_frequent:__mapping__").first()
                mapping = json.loads(map_row.mapping_json) if (map_row and map_row.mapping_json) else {}
                df = pd.DataFrame()
                for r in rp_rows:
                    if not r.data_json:
                        continue
                    d = _json_to_df(r.data_json)
                    if not d.empty:
                        d = _apply_mapping(d, mapping)
                        try:
                            d['_ุงููุชุฑุฉ'] = 'ุงูุจูุงูุงุช ุงูุญุฏูุซุฉ (ุงูุจุฑูุงูุฌ)'
                        except Exception:
                            pass
                        df = pd.concat([df, d], ignore_index=True) if not df.empty else d
                result_df = _drop_empty_columns(df)
                meta = {'month_label': None, 'year_label': None, 'recent_program': True, 'source': 'recent_program'}
                try:
                    _VISIT_CACHE = {'df': result_df.copy(), 'meta': meta.copy(), 'ts': time()}
                except Exception:
                    pass
                return result_df, meta
            except Exception:
                return pd.DataFrame(), {'month_label': None, 'year_label': None, 'source': 'month'}
        except Exception:
            # ูู ุฃู ุฎุทุฃุ ูุนูุฏ ูุฅุณุชุฑุงุชูุฌูุฉ ูุฏููุฉ (ุฅู ููุฌุฏุช) ุฃู ููุฑุฌุน ูุงุฑุบ
            try:
                row_vis = _load_state("visit_history")
                if not row_vis or not row_vis.data_json:
                    return pd.DataFrame(), {'month_label': None, 'year_label': None, 'source': 'month'}
                df_vis = _json_to_df(row_vis.data_json)
                map_row = _load_state("visit_history:__mapping__")
                mapping = json.loads(map_row.mapping_json) if (map_row and map_row.mapping_json) else {}
                df_vis = _apply_mapping(df_vis, mapping)
                return _drop_empty_columns(df_vis), {'month_label': None, 'year_label': None, 'source': 'month'}
            except Exception:
                return pd.DataFrame(), {'month_label': None, 'year_label': None, 'source': 'month'}

    def _detect_visit_columns(cols: list[str]) -> tuple[str|None, str|None, str|None, str|None]:
        # ุงูุชุดุงู ุฃุนูุฏุฉ: ูุณูุณูุ ุงูุชุงุฑูุฎุ ุฑูู ุงูุนูููุ ุงุณู ุงูุนููู (ุงุฎุชูุงุฑูุฉ)
        serial_candidates = ['ูุณูุณู', 'ูุณูุณู ุงููุงูููุฉ', 'Serial', 'POS Serial', 'POS', 'ุฑูู ุงููุงูููุฉ']
        date_candidates = ['ุงูุชุงุฑูุฎ', 'ุชุงุฑูุฎ ุงูุฒูุงุฑุฉ', 'Date', 'Visit Date', 'ุชุงุฑูุฎ']
        customer_code_candidates = ['ุฑูู ุงูุนููู', 'Customer Code', 'Customer_ID', 'client_code', 'trader_id', 'code', 'ID', 'id', 'Bakery ID', 'ุฑูู ุงููุฎุจุฒ', 'ุฑูู ุงูุชุงุฌุฑ']
        customer_name_candidates = ['ุงุณู ุงูุนููู', 'ุงุณู ุงููุฎุจุฒ', 'ุงุณู ุงูุชุงุฌุฑ', 'Customer Name', 'Trader Name', 'Bakery Name', 'trader_name', 'name', 'customer_name', 'trader_name_ar']
        def pick(cands):
            for c in cands:
                if c in cols:
                    return c
            return None
        return (
            pick(serial_candidates),
            pick(date_candidates),
            pick(customer_code_candidates),
            pick(customer_name_candidates)
        )

    visit_history_df, _period_meta = _load_visit_history_df()

    # ุฌูุน ูุณูุณูุงุช ุงููุฌููุนุฉ ุงูุญุงููุฉ ูุงุณุชุฎุฏุงููุง ูู ุงูุชุตููุฉ ุฅุฐุง ูู ูุชููุฑ ุฑูู ุงูุนููู ูู ุณุฌู ุงูุฒูุงุฑุงุช
    current_serials = set()
    try:
        for md in first_group.get('machine_details', []):
            if 'ูุณูุณู ุงููุงูููุฉ' in md and _textify(md.get('ูุณูุณู ุงููุงูููุฉ')) != '':
                current_serials.add(_textify(md.get('ูุณูุณู ุงููุงูููุฉ')))
    except Exception:
        current_serials = set()

    visit_debug = {}
    # ุฅุถุงูุฉ ูุนูููุงุช ูุทุงุจูุฉ ุงููุงูููุงุช ุงูุฃุณุงุณูุฉ ูููุญุฉ ุงูุชุดุฎูุต
    try:
        visit_debug['primary'] = primary_debug
    except Exception:
        pass
    if not visit_history_df.empty:
        # ุฅุนุงุฏุฉ ุจูุงุก ุงูููุทู: ุชูุญูุฏ ุงูุฃุนูุฏุฉ ุซู ูุทุงุจูุฉ ุตุงุฑูุฉ ููุฑูุฉ
        dfv = _standardize_visit_df(visit_history_df)
        visit_debug['source_cols'] = list(visit_history_df.columns)
        visit_debug['std_cols'] = list(dfv.columns)
        visit_debug['source_rows'] = int(visit_history_df.shape[0])
        code_norm = _norm_key_text(str(customer_code)) if _textify(customer_code) != '' else None
        name_norm = _norm_key_text(str(customer_name)) if _textify(customer_name) != '' else None
        serials_norm = set(_norm_key_text(s) for s in current_serials) if current_serials else set()
        # ููุชุฑุฉ ุญุณุจ ุงูููุน ุงููุทููุจ (ูุฎุงุจุฒ/ุชูููู/ุงุณุชุจุฏุงู) ูุจู ุจูุงุก ุงูุฃููุนุฉ
        pre_dfv = dfv
        target_type = CATEGORIES.get(category, category)
        type_norm = _norm_key_text(str(target_type)) if _textify(target_type) != '' else None
        type_mask = None
        if ('ุงูููุน' in pre_dfv.columns) and type_norm is not None:
            try:
                type_mask = (pre_dfv['ุงูููุน'].apply(_textify).apply(_norm_key_text) == type_norm)
            except Exception:
                type_mask = None
        if type_mask is not None:
            pre_dfv = pre_dfv[type_mask]
        visit_debug['type_norm'] = type_norm
        visit_debug['type_rows'] = int(pre_dfv.shape[0])

        mask = None
        # ุงุณุชุฎุฏุงู _textify ูุจู _norm_key_text ูุถูุงู ุฅุฒุงูุฉ ุงูุฃุฌุฒุงุก ูุซู ".0" ูุชุญููู ุงูุฃุฑูุงู ุฅูู ูุต ููุญุฏ
        if 'ุฑูู ุงูุนููู' in dfv.columns and code_norm is not None:
            mask = (dfv['ุฑูู ุงูุนููู'].apply(_textify).apply(_norm_key_text) == code_norm)
        if 'ุงุณู ุงูุนููู' in dfv.columns and name_norm is not None:
            nm = (dfv['ุงุณู ุงูุนููู'].apply(_textify).apply(_norm_key_text) == name_norm)
            mask = (mask & nm) if mask is not None else nm
        # ุทุจูู ุงูุชูุงุทุน ุฅู ุชููุฑ ููุง ุงูุนููุฏูู
        if mask is not None:
            dfv = dfv[mask]
            # ุจุนุฏ ุชุทุจูู ุงูุชูุงุทุนุ ุทุจูู ููุชุฑุฉ ุงูููุน ุฅู ููุฌุฏุช
            if type_mask is not None:
                dfv = dfv[type_mask]
        # ุฅู ูู ุชููุชุฌ ูุชูุฌุฉุ ุฌุฑูุจ ุงุชุญุงุฏ ุงูุดุฑุทูู ุฅู ูุงูุง ูุชููุฑูู
        if dfv.empty and ('ุฑูู ุงูุนููู' in pre_dfv.columns or 'ุงุณู ุงูุนููู' in pre_dfv.columns):
            union_mask = None
            if 'ุฑูู ุงูุนููู' in pre_dfv.columns and code_norm is not None:
                union_mask = (pre_dfv['ุฑูู ุงูุนููู'].apply(_textify).apply(_norm_key_text) == code_norm)
            if 'ุงุณู ุงูุนููู' in pre_dfv.columns and name_norm is not None:
                nm = (pre_dfv['ุงุณู ุงูุนููู'].apply(_textify).apply(_norm_key_text) == name_norm)
                union_mask = (union_mask | nm) if union_mask is not None else nm
            if union_mask is not None:
                dfv = pre_dfv[union_mask]
        # ูุฅู ุจููุช ูุงุฑุบุฉุ ุงุนุชูุฏ ุงููุณูุณูุงุช ูุญู ุฃุฎูุฑ
        if dfv.empty and ('ูุณูุณู' in pre_dfv.columns) and serials_norm:
            dfv = pre_dfv[pre_dfv['ูุณูุณู'].apply(_textify).apply(_norm_key_text).isin(serials_norm)]
        # ุณุฌูู ุนุฏู ููุตูู ููุฑูู ููููุณูุณู ูุน ููุชุฑุฉ ุงูููุน
        try:
            code_count = 0
            if ('ุฑูู ุงูุนููู' in pre_dfv.columns) and (code_norm is not None):
                code_count = int((pre_dfv['ุฑูู ุงูุนููู'].apply(_textify).apply(_norm_key_text) == code_norm).sum())
            serial_count = 0
            if ('ูุณูุณู' in pre_dfv.columns) and serials_norm:
                serial_count = int(pre_dfv['ูุณูุณู'].apply(_textify).apply(_norm_key_text).isin(serials_norm).sum())
            visit_debug['code_count'] = code_count
            visit_debug['serial_count'] = serial_count
        except Exception:
            pass
        visit_debug['code_norm'] = code_norm
        visit_debug['name_norm'] = name_norm
        visit_debug['serials_norm_count'] = len(serials_norm)
        visit_debug['matched_rows'] = int(dfv.shape[0])
        visit_debug['count_mode'] = 'textify_norm_matching'
        # ููุงุญุธุฉ: ุณููุฑุฑ ุฅุทุงุฑ ุงูุจูุงูุงุช ุงูููุญูุฏ ุจุงููุงูู ุฅูู ุฏุงูุฉ ุงูุนุฏ ุงูุชู ุชุชุนุงูู ูุน ูุฌูุฏ/ุบูุงุจ ุงูุชุงุฑูุฎ ูุงููุณูุณู.
        # ุชุญุฏูุฏ ุงููุชุฑุฉ ุชููุงุฆูุงู ูู ุงูููุชุงุฏุงุชุง: ุงูุณูุฉ โ yearุ ุงูุญุฏูุซุฉ โ recent_programุ ูุฅูุง ุดูุฑ
        if isinstance(_period_meta, dict) and _period_meta.get('source') in ['year','month','recent_program']:
            if _period_meta.get('source') == 'year':
                visit_period = 'year'
            elif _period_meta.get('source') == 'recent_program':
                visit_period = 'recent_program'
            else:
                visit_period = 'month'
        visit_data = _fetch_visit_data(customer_code, dfv, visit_period, _period_meta.get('month_label'), _period_meta.get('year_label')) if (not dfv.empty) else {'current_month': {'total': 0, 'details': {}}, 'current_year': {'total': 0, 'details': {}}}
        visit_debug['month_total'] = int(visit_data['current_month']['total'])
        visit_debug['year_total'] = int(visit_data['current_year']['total'])
    else:
        # ูุง ููุฌุฏ ุณุฌู ุฒูุงุฑุงุช ูุฎุฒู
        visit_data = {'current_month': {'total': 0, 'details': {}}, 'current_year': {'total': 0, 'details': {}}}
        visit_debug['reason'] = 'empty_visit_history'

    # ุฅูุญุงู ุชุดุฎูุต ุงููุงูููุงุช ุงูุฃุณุงุณูุฉ ุถูู ุชุดุฎูุต ุงูุฒูุงุฑุงุช ูุนุฑุถู ูู ุงููุงุฌูุฉ
    if isinstance(visit_debug, dict):
        visit_debug['primary'] = primary_debug

    # 11. ุฅุฑุฌุงุน ุงููููู ุงููุทููุจ ูู ุงููุงุฌูุฉ
    return {
        'success': True,
        'message': message,
        'visit_period': visit_period,
        'customer_data': customer_data,
        'dynamic_fields': dynamic_fields,
        'primary_record': primary_record,
        'primary_match_mode': primary_match_mode,
        'branch_section': branch_section,
        'visit_data': visit_data,
        'visit_debug': visit_debug,
        'serial_list': serial_list,
        'cols': list(mapped_df.columns),
        # ุฅุถุงูุฉ ูุชูุฌุฉ ุงูุชุฌููุน ุจุงููุงูู ููุณูุงุญ ููู JS ุจุงูุชุนุงูู ูุน ุงูููุงูุงุช ุงููุชุนุฏุฏุฉ ุฅุฐุง ูุฒู ุงูุฃูุฑ
        'items': grouped_nested_results, 
    }

@machine_reports_bp.route('/inquiry', methods=['GET'])
@login_required
@role_required(['admin', 'data_entry', 'user'])
@permission_required('can_inquiry')
def inquiry_view():
    """ุนุฑุถ ุตูุญุฉ ุงูุงุณุชุนูุงู"""
    try:
        from models import User
        maintenance_names = [u.username for u in User.query.order_by(User.username).all()]
    except Exception:
        maintenance_names = []
    return render_template('inquiry_popup.html', categories=CATEGORIES, maintenance_names=maintenance_names)


@machine_reports_bp.route('/api/inquiry_search', methods=['POST'])
@login_required
@role_required(['admin', 'data_entry', 'user'])
@permission_required('can_inquiry')
def api_inquiry_search():
    """ูุงุฌูุฉ API ูุชูููุฐ ุงูุจุญุซ ุจูุงุกู ุนูู ูุฏุฎูุงุช ุงููุณุชุฎุฏู"""
    data = request.json
    category = data.get('category')
    search_type = data.get('search_type')
    query = data.get('query')
    visit_period = (data.get('visit_period') or 'recent_program').strip()

    if category not in CATEGORIES or search_type not in ['code', 'serial', 'name', 'machine_code'] or not query:
        return jsonify({'success': False, 'message': 'ุจูุงูุงุช ุจุญุซ ุบูุฑ ุตุงูุญุฉ.', 'items':[]}), 400

    if visit_period not in ['month','year','recent_program']:
        visit_period = 'recent_program'
    result = _inquiry_search(category, search_type, query, visit_period)

    return jsonify(result)


@machine_reports_bp.route('/api/service_tickets/save', methods=['POST'])
@login_required
@role_required(['admin', 'data_entry', 'user'])
def api_save_service_tickets():
    """ุญูุธ ุชุฐุงูุฑ ุงูุฎุฏูุงุช ุงููุฑุชุจุทุฉ ุจูู ูุณูุณู ุญุณุจ ุชุจููุจ ุงูุงุณุชุนูุงู."""
    payload = request.json or {}
    category_key = payload.get('category')
    tickets = payload.get('tickets') or []
    customer_data = payload.get('customer_data') or {}

    if category_key not in CATEGORIES:
        return jsonify({'success': False, 'message': 'ูุณู ุบูุฑ ุตุงูุญ.', 'errors': ['ูุณู ุบูุฑ ุตุงูุญ']}), 400

    if not isinstance(tickets, list) or len(tickets) == 0:
        return jsonify({'success': False, 'message': 'ูุง ุชูุฌุฏ ุณุฌูุงุช ููุญูุธ.', 'errors': ['ูุง ุชูุฌุฏ ุณุฌูุงุช']}), 400

    errors = []
    now = datetime.utcnow()
    category_label = CATEGORIES.get(category_key, category_key)

    # ุชุญูู ุฑูู ุงูุฅุฐู: ุฃุฑูุงู ููุท ูููู ุฅูุฒุงูู ููุท ุฅุฐุง ุชู ุงุฎุชูุงุฑ ููุน/ุฃููุงุน ุนุทู
    local_orders = []
    for i, t in enumerate(tickets):
        raw_fts = t.get('fault_types')
        fts = []
        if isinstance(raw_fts, list):
            fts = [x.strip() for x in raw_fts if x and str(x).strip()]
        single_ft = (t.get('fault_type') or '').strip()
        has_fault = (len(fts) > 0) or bool(single_ft)

        on = str(t.get('order_number','')).strip()
        if has_fault:
            # ุฅุฐุง ูุงู ููุงู ุนุทูุ ูุฌุจ ุฃู ูููู ุฑูู ุงูุฅุฐู ููุฌูุฏูุง ูุฃุฑูุงู ููุท
            if not on or not on.isdigit():
                errors.append(f"ุณุทุฑ {i+1}: ุฑูู ุงูุฅุฐู ูุฌุจ ุฃู ูููู ุฃุฑูุงู ููุท ููุทููุจ ุนูุฏ ุชุณุฌูู ุนุทู.")
        # ูุฌูุน ุงูุฃูุงูุฑ ููุท ูููุฏูู ุงูุฎุงุต ุจุชูุฑุงุฑ ุงูุฑูู ุนุจุฑ ุงูุนููุงุก ุงููุฎุชูููู
        if on:
            local_orders.append(on)

    # ุชุญูู ูู ุฃููุงุน ุงูุฃุนุทุงู (ูุฏุนู ูุชุนุฏุฏุฉ) + ุงูุณูุงุญ ุจุนุทู ููุงูููุฉ ูุงุญุฏุฉ ููุท
    fault_rows_count = 0
    for i, t in enumerate(tickets):
        # ุชูุธูู ุงูููู ุงููุงุฑุบุฉ ูู ุงูุฃุนุทุงู ุงููุชุนุฏุฏุฉ ูุจู ุงูุชุญูู
        raw_fts = t.get('fault_types')
        fts = []
        if isinstance(raw_fts, list):
            fts = [x.strip() for x in raw_fts if x and str(x).strip()]

        if isinstance(fts, list) and len(fts) > 0:
            fault_rows_count += 1
            bads = [x for x in fts if x not in ALLOWED_FAULT_TYPES]
            if bads:
                errors.append(f"ุณุทุฑ {i+1}: ููุน/ุฃููุงุน ุนุทู ุบูุฑ ูุณููุญ ({', '.join(bads)}).")
        else:
            # ูู ุญุงูุฉ ุนุฏู ุงุฎุชูุงุฑ ุฃุนุทุงู ูุชุนุฏุฏุฉุ ูุนูุฏ ููุชุญูู ูู ุญูู ูุงุญุฏ ุงุฎุชูุงุฑู
            ft = (t.get('fault_type') or '').strip()
            if ft and ft not in ALLOWED_FAULT_TYPES:
                errors.append(f"ุณุทุฑ {i+1}: ููุน ุนุทู ุบูุฑ ูุณููุญ ({ft}).")
            if ft:
                fault_rows_count += 1

    # ุงูุณูุงุญ ุจุชุณุฌูู ุฃุนุทุงู ูุนุฏุฉ ูุงูููุงุช: ุฅุฒุงูุฉ ุงูููุฏ ุงูุณุงุจู
    # ูููู ูููุณุชุฎุฏู ุชุณุฌูู ุฃูุซุฑ ูู ุนุทู ูู ููุณ ุงูุนูููุฉ ุฏูู ุชูููุฏ ุจุนุฏุฏ ูุงุญุฏ

    # ุงูุณูุงุญ ุจุชูุฑุงุฑ ุฑูู ุงูุฅุฐู ููุท ุฅุฐุง ุชุทุงุจู "ุฑูู ุงูุนููู" ู"ุงุณู ุงูุนููู" ูุน ุงูุณุฌู ุงูุณุงุจู
    customer_code = customer_data.get('ุฑูู ุงูุนููู') or customer_data.get('ุฑูู ุงููุฎุจุฒ') or ''
    customer_name = customer_data.get('ุงุณู ุงูุนููู') or customer_data.get('ุงุณู ุงููุฎุจุฒ') or ''
    if not errors and local_orders:
        try:
            existing_rows = (
                ServiceTicket.query
                .filter(ServiceTicket.order_number.in_(local_orders))
                .all()
            )
            for row in existing_rows:
                same_code = (row.customer_code or '') == (customer_code or '')
                same_name = (row.customer_name or '') == (customer_name or '')
                if not (same_code and same_name):
                    errors.append(f"ุฑูู ุงูุฅุฐู ูุณุชุฎุฏู ูุนููู ุขุฎุฑ: {row.order_number}")
        except Exception:
            pass

    if errors:
        return jsonify({'success': False, 'message': 'ูุดู ุงูุญูุธ ุจุณุจุจ ุฃุฎุทุงุก.', 'errors': errors}), 400

    # ุญูุธ ุงูุณุฌูุงุช
    try:
        saved_payload_rows = []
        for t in tickets:
            faults_list = t.get('fault_types') or []
            fault_str = ','.join([x.strip() for x in faults_list if x and x.strip()]) if faults_list else (t.get('fault_type') or '').strip()
            # ูุง ูุญูุธ ุงูุณุทุฑ ุฅุฐุง ูู ููุณุฌููู ุฃู ุนุทู
            if not fault_str:
                continue
            st = ServiceTicket(
                created_at=now,
                category_key=category_key,
                category_label=category_label,
                fault_type=fault_str,
                order_number=str(t.get('order_number') or '').strip(),
                username=getattr(current_user, 'username', 'unknown'),
                customer_code=customer_code,
                customer_name=customer_data.get('ุงุณู ุงูุนููู') or customer_data.get('ุงุณู ุงููุฎุจุฒ') or '',
                machine_code=t.get('machine_code') or '',
                machine_serial=t.get('machine_serial') or '',
                main_sub=t.get('main_sub') or '',
                status=t.get('status') or '',
                sim1=t.get('sim1') or '',
                sim2=t.get('sim2') or '',
                services=getattr(current_user, 'username', 'unknown'),
                maintenance=str(t.get('maintenance') or '').strip()
            )
            db.session.add(st)
            # ูุจูู ุตููุง ููุชุฑุญูู ุฅูู ุฎุฏูุงุช ุงูุชุฌุงุฑ (ุงููุชุฑุฏุฏูู - ุงูุจูุงูุงุช ุงูุญุฏูุซุฉ)
            # ุฅูุดุงุก ุตู ุงูุจูุงูุงุช ุงูุฃุณุงุณูุฉ
            row_data = {
                'ุงูุชุงุฑูุฎ': now.strftime('%Y-%m-%d %H:%M:%S'),
                'ุงููุณู': category_label,
                'ุงูุงุฏุงุฑุฉ': customer_data.get('ุงูุงุฏุงุฑุฉ') or '',
                'ุงูููุชุจ': customer_data.get('ุงูููุชุจ') or '',
                'ุฑูู ุงูุนููู': customer_code,
                'ุงุณู ุงูุนููู': customer_data.get('ุงุณู ุงูุนููู') or customer_data.get('ุงุณู ุงููุฎุจุฒ') or '',
                'ุฑูู ุงููุงูููุฉ': t.get('machine_code') or '',
                'ูุณูุณู': t.get('machine_serial') or '',
                'ุฑุฆูุณูุฉ/ูุฑุนูุฉ': t.get('main_sub') or '',
                'ุญุงูุฉ ุงููุงูููุฉ': t.get('status') or '',
                'ุดุฑูุญุฉ1': t.get('sim1') or '',
                'ุดุฑูุญุฉ2': t.get('sim2') or '',
                'ุฑูู ุงูุฅุฐู': str(t.get('order_number') or '').strip(),
                'ุงูุญูุงูุฉ ุงููุทููุจุฉ': str(t.get('required_transfer') or '').strip(),
                'ุงููุงุฆู ุจุงูุตูุงูุฉ': str(t.get('maintenance') or '').strip(),
                'ุงุณู ุงููุณุชุฎุฏู': getattr(current_user, 'username', 'unknown'),
                'ุฎุฏูุงุช': getattr(current_user, 'username', 'unknown'),
                'ููุงุญุธุงุช1': str(t.get('notes1') or '').strip()
            }
            
            # ุฅุถุงูุฉ ุฃุนูุฏุฉ ุงูุฃุนุทุงู ุงููููุตูุฉ ูุน ุงูููู ุงูุงูุชุฑุงุถูุฉ ูุงุฑุบุฉ
            fault_types_in_ticket = [x.strip() for x in faults_list if x and x.strip()]
            for fault_type in ALLOWED_FAULT_TYPES:
                row_data[fault_type] = '1' if fault_type in fault_types_in_ticket else ''
            
            saved_payload_rows.append(row_data)
        
        # ุงูุชุญูู ูู ุตุญุฉ ุงูุจูุงูุงุช ูุจู ุงูุญูุธ ุงูููุงุฆู
        db.session.flush()
        db.session.commit()

        # ุชุฑุญูู ุงูุจูุงูุงุช ุฅูู ูุณู ุฎุฏูุงุช ุงูุชุฌุงุฑ โ ุงููุชุฑุฏุฏูู: ุงูุจูุงูุงุช ุงูุญุฏูุซุฉ (ุงูุจุฑูุงูุฌ)
        try:
            if saved_payload_rows:
                import pandas as pd
                from models_reports import ReportState
                # ุชุญููู ุฅูู DataFrame ุจููุณ ุชูุณูู ุงูุชูุงุฑูุฑ
                df = pd.DataFrame(saved_payload_rows)

                # ุชุทุจูู ููุชุฑุฉ ุงูุณุฌูุงุช: ุงูุงุญุชูุงุธ ููุท ุจูุง ูุญุชูู ุนูู ุฃุนุทุงู ุฃู ุชุงุฑูุฎ ุฃู ุฎุฏูุงุช ุฃู ุตูุงูุฉ
                fault_cols = [c for c in ALLOWED_FAULT_TYPES if c in df.columns]
                has_fault = df[fault_cols].eq('1').any(axis=1) if fault_cols else pd.Series([False] * len(df))

                def _nonempty(df, col):
                    return df[col].astype(str).str.strip() != '' if col in df.columns else pd.Series([False] * len(df))

                mask = has_fault | _nonempty(df, 'ุงููุงุฆู ุจุงูุตูุงูุฉ') | _nonempty(df, 'ุฎุฏูุงุช') | _nonempty(df, 'ุงูุชุงุฑูุฎ')
                df_filtered = df[mask]

                # ุฅุฒุงูุฉ ุงูุชูุฑุงุฑ ุงุนุชูุงุฏูุง ุนูู "ุฑูู ุงูุฅุฐู" ุฅู ูุฌุฏุ ูุฅูุง ุนูู ุงูุตู ุจุงููุงูู
                if not df_filtered.empty:
                    if 'ุฑูู ุงูุฅุฐู' in df_filtered.columns:
                        df_filtered = df_filtered.drop_duplicates(subset=['ุฑูู ุงูุฅุฐู'], keep='last')
                    else:
                        df_filtered = df_filtered.drop_duplicates(keep='last')

                # ุญูุธ/ุชุญุฏูุซ ุณุฌู ุงูุญุงูุฉ ููุฃุฏูู ุงูุญุงูู ูุน ุงูุฏูุฌ ุงูุชุฑุงููู ูุณุฌูุงุช ุดุงุดุฉ ุงูุงุณุชุนูุงู ููุท
                user_id = getattr(current_user, 'id', None)
                row = ReportState.query.filter_by(category='trader_frequent:recent_program', user_id=user_id).first()
                if not row:
                    row = ReportState(category='trader_frequent:recent_program', user_id=user_id)
                    db.session.add(row)
                    existing_df = pd.DataFrame()
                else:
                    try:
                        existing_df = _json_to_df(row.data_json) if row.data_json else pd.DataFrame()
                    except Exception:
                        existing_df = pd.DataFrame()

                # ุงูููู ูุจุฏุฃ ูุงุฑุบูุงุ ูุฃูู ุญูุธ ูุถูู ุฃูู ุณุฌูุ ุซู ุชุชุฑุงูู ุงูุณุฌูุงุช ุงูุฌุฏูุฏุฉ ููุท
                combined = pd.concat([existing_df, df_filtered], ignore_index=True) if not existing_df.empty else df_filtered
                if not combined.empty:
                    if 'ุฑูู ุงูุฅุฐู' in combined.columns:
                        combined = combined.drop_duplicates(subset=['ุฑูู ุงูุฅุฐู'], keep='last')
                    else:
                        combined = combined.drop_duplicates(keep='last')

                row.data_json = _df_to_json(combined) if not combined.empty else _df_to_json(pd.DataFrame())
                db.session.commit()
        except Exception as ex2:
            # ูุง ููุดู ุงูุญูุธ ุงูุฑุฆูุณู ุจุณุจุจ ูุดููุฉ ุชุฑุญูู ุงูุจูุงูุงุช ุงููุณุงุนุฏุฉ
            current_app = None
            try:
                from flask import current_app as _ca
                current_app = _ca
            except Exception:
                pass
            try:
                if current_app:
                    current_app.logger.warning(f'Trader frequent recent_program sync warning: {ex2}')
            except Exception:
                pass
    except Exception as ex:
        db.session.rollback()
        return jsonify({'success': False, 'message': f'ูุดู ุงูุญูุธ: {ex}', 'errors': [str(ex)]}), 500

    return jsonify({'success': True, 'message': 'ุชู ุญูุธ ุงูุณุฌูุงุช ุจูุฌุงุญ.', 'saved': len(tickets)})

@machine_reports_bp.route('/api/recent_program/reset', methods=['POST'])
@login_required
@role_required(['admin'])
def api_reset_recent_program():
    """ุชูุฑูุบ ุฌุฏูู recent_program ูุฅูุดุงุคู ุจููุณ ุฃุณูุงุก ุงูุฃุนูุฏุฉ ูููู ูุงุฑุบ ูุจุฏุก ุงุณุชูุจุงู ุงูุจูุงูุงุช ุงูุฌุฏูุฏุฉ."""
    try:
        import pandas as pd
        from models_reports import ReportState

        # ุงูุฃุนูุฏุฉ ุงูุฃุณุงุณูุฉ ุงููุณุชุฎุฏูุฉ ูู ุดุงุดุฉ ุงูุงุณุชุนูุงู + ุฃุนูุฏุฉ ุงูุฃุนุทุงู
        base_cols = [
            'ุงูุงุฏุงุฑุฉ','ุงูููุชุจ','ุฑูู ุงูุนููู','ุงุณู ุงูุนููู','ุฑูู ุงููุงูููุฉ','ูุณูุณู',
            'ุฑุฆูุณูุฉ/ูุฑุนูุฉ','ุญุงูุฉ ุงููุงูููุฉ','ุดุฑูุญุฉ1','ุดุฑูุญุฉ2','ุฑูู ุงูุฅุฐู',
            'ุงูุญูุงูุฉ ุงููุทููุจุฉ','ุงููุงุฆู ุจุงูุตูุงูุฉ','ุงุณู ุงููุณุชุฎุฏู','ุฎุฏูุงุช','ููุงุญุธุงุช1'
        ]
        fault_cols = list(ALLOWED_FAULT_TYPES)
        cols = base_cols + fault_cols

        # ุฅูุดุงุก DataFrame ูุงุฑุบ ุจูุฐู ุงูุฃุนูุฏุฉ
        df_empty = pd.DataFrame(columns=cols)

        # ุชุญุฏูุซ ุณุฌู ุงูุญุงูุฉ ูููุณุชุฎุฏู ุงูุญุงูู
        user_id = getattr(current_user, 'id', None)
        row = ReportState.query.filter_by(category='trader_frequent:recent_program', user_id=user_id).first()
        if not row:
            row = ReportState(category='trader_frequent:recent_program', user_id=user_id)
            db.session.add(row)

        row.data_json = _df_to_json(df_empty)
        db.session.commit()

        return jsonify({'success': True, 'message': 'ุชู ุชูุฑูุบ recent_program ูุฅูุดุงุคู ูุงุฑุบูุง ุจููุณ ุงูุฃุนูุฏุฉ.'})
    except Exception as ex:
        db.session.rollback()
        return jsonify({'success': False, 'message': f'ูุดู ุงูุชูุฑูุบ: {ex}'}), 500

@machine_reports_bp.route("/")
@login_required
@permission_required('can_general_reports')
def index():
    return render_template("reports/index.html", categories=CATEGORIES, title="ุงูุชูุงุฑูุฑ ุงูุนุงูุฉ")

@machine_reports_bp.route("/<category>", methods=["GET"])
@login_required
@permission_required('can_general_reports')
def category_view(category):
    if category not in CATEGORIES:
        flash("ูุณู ุบูุฑ ููุฌูุฏ", "warning")
        return redirect(url_for("machine_reports_bp.index"))

    row = _load_state(category)
    is_admin = getattr(current_user, "role", None) == "admin"

    df = _json_to_df(row.data_json) if row and row.data_json else pd.DataFrame()
    mapping = json.loads(row.mapping_json) if row and row.mapping_json else {}

    q = request.args.get("q", "", type=str)
    search_in = request.args.get("search_in", "all")
    page = request.args.get("page", 1, type=int)
    page_size = request.args.get("page_size", 25, type=int)
    page_size = 10 if page_size < 10 else 1000 if page_size > 1000 else page_size

    search_cols_for_view = [search_in] if search_in != 'all' else None

    if not df.empty:
        mapped    = _apply_mapping(df, mapping)
        filtered = _filter_dataframe(mapped, q, search_cols=search_cols_for_view)
        visible  = _drop_empty_columns(filtered)
        
        if visible.empty:
            page_df = pd.DataFrame(); total_pages = 1; search_cols = []
        else:
            page_df, total = _paginate(visible, page, page_size)
            total_pages = max(1, (total + page_size - 1)//page_size)
            search_cols = list(visible.columns)
    else:
        page_df = pd.DataFrame(); total_pages = 1; search_cols = []

    cols = list(page_df.columns)
    rows = page_df.to_dict(orient="records") if not page_df.empty else [] 

    def _page_url(n):
        return url_for('machine_reports_bp.category_view', category=category, q=q, search_in=search_in, page=n, page_size=page_size)
    first_pages = [n for n in [1,2,3] if n <= total_pages]
    pagination = {
        "prev": _page_url(page-1) if page>1 else None,
        "next": _page_url(page+1) if page<total_pages else None,
        "first_pages": [{"n":n,"url":_page_url(n),"active":(n==page)} for n in first_pages],
        "show_ellipsis": total_pages > 3,
        "last": {"n":total_pages,"url":_page_url(total_pages),"active":(page==total_pages)} if total_pages>3 else None,
        "page": page, "total_pages": total_pages
    }

    try:
        # ุงูุชุญูู ููุง ุฅุฐุง ูุงูุช ุงูุฏุงูุฉ ููุฌูุฏุฉ ููุณูุงุญ ุจุชูููู/ุชุนุทูู ุฒุฑ ุงูุญูุธ
        _ = url_for('machine_reports_bp.save_mapping', category=category) 
        mapping_enabled = True
    except Exception:
        mapping_enabled = False

    return render_template("reports/category.html",
                           categories=CATEGORIES,
                           category=category,
                           category_label=CATEGORIES[category],
                           has_data=(not df.empty),
                           q=q, search_in=search_in, page=page, page_size=page_size,
                           cols=cols, rows=rows,
                           order_csv=",".join(mapping.get("order", [])),
                           rename_lines="\n".join([f"{k}=>{v}" for k,v in (mapping.get('rename') or {}).items()]),
                           is_admin=is_admin, mapping_enabled=mapping_enabled,
                           pagination=pagination, search_cols=search_cols)

@machine_reports_bp.route("/<category>/import_view", methods=["GET"])
@login_required
@role_required(['admin', 'data_entry'])
@permission_required('can_general_reports')
def import_view(category):
    if category not in CATEGORIES:
        flash("ูุณู ุบูุฑ ููุฌูุฏ", "warning")
        return redirect(url_for("machine_reports_bp.index"))
    return render_template("reports/import.html",
                           categories=CATEGORIES,
                           category=category,
                           category_label=CATEGORIES[category])

@machine_reports_bp.route("/<category>/save_mapping", methods=["POST"])
@login_required
@role_required(['admin']) 
def save_mapping(category):
    if category not in CATEGORIES:
        flash("ูุณู ุบูุฑ ููุฌูุฏ", "warning")
        return redirect(url_for("machine_reports_bp.index"))

    order_csv = (request.form.get("order_csv") or "").strip()
    order = [c.strip() for c in order_csv.split(",") if c.strip()] if order_csv else []

    rename_lines = (request.form.get("rename_lines") or "").strip()
    rename = {}
    if rename_lines:
        for line in rename_lines.splitlines():
            if "=>" in line:
                old, new = line.split("=>", 1)
                old, new = old.strip(), new.strip()
                if old and new:
                    rename[old] = new

    mapping = {"order": order, "rename": rename}
    _save_state(category, df=None, mapping=mapping)
    # ุชุญุฏูุซ ุงููุงุด ุจุนุฏ ุชุบููุฑ ุงููุงุจูุฌ
    try:
        _invalidate_inquiry_cache(category)
    except Exception:
        pass
    flash("ุชู ุญูุธ ุฅุนุฏุงุฏุงุช ุงููุงุจูุฌ.", "success")
    return redirect(url_for("machine_reports_bp.category_view", category=category))

@machine_reports_bp.route("/<category>/import", methods=["POST"])
@login_required
@role_required(['admin', 'data_entry'])
@permission_required('can_general_reports')
def import_files(category):
    if category not in CATEGORIES:
        flash("ูุณู ุบูุฑ ููุฌูุฏ", "warning")
        return redirect(url_for("machine_reports_bp.index"))

    # 1. ุชุฌููุน ุงููููุงุช ุงููุฑููุนุฉ ููุนูููุงุชูุง
    uploaded_files_info = []
    for i in range(1, MAX_FILES + 1):
        file_storage = request.files.get(f"file{i}")
        if file_storage and file_storage.filename:
            uploaded_files_info.append({"file_storage": file_storage, "filename": file_storage.filename, "index": i})

    if len(uploaded_files_info) == 0:
        flash("ุจุฑุฌุงุก ุงุฎุชูุงุฑ ููู (ุงููุณูุณูุงุช) ุนูู ุงูุฃูู.", "warning")
        return redirect(url_for("machine_reports_bp.import_view", category=category))

    # 2. ูุฑุงุกุฉ ุงููููุงุช ูุชุฌููุน ุงูุจูุงูุงุช ุงูุชู ุชู ูุฑุงุกุชูุง ุจูุฌุงุญ
    dfs_all = [] 
    successful_files = []
    failed_filenames = []
    
    for item in uploaded_files_info:
        try:
            df = _read_any(item["file_storage"])
            dfs_all.append(df)
            if df.empty:
                failed_filenames.append(f'{item["filename"]} (ุงููููุน: ููู {item["index"]} - ูุงุฑุบ/ูุดู ูู ุงููุฑุงุกุฉ)')
            else:
                successful_files.append(item["filename"])
        except Exception as ex:
             current_app.logger.exception(f"Error reading file {item['filename']}: {ex}")
             dfs_all.append(pd.DataFrame()) 
             failed_filenames.append(f'{item["filename"]} (ุงููููุน: ููู {item["index"]} - ูุดู ุญุงุฏ ูู ุงููุฑุงุกุฉ)')


    # 3. ุงูุฏูุฌ
    try:
        out_df = _merge_all(dfs_all, category)
        
        if out_df.empty and successful_files:
             flash("ุชู ูุฑุงุกุฉ ุงููููุงุชุ ููู ุนูููุฉ ุงูุฏูุฌ ูู ุชูุชุฌ ุนููุง ุณุฌูุงุช ุตุงูุญุฉ.", "warning")
             return redirect(url_for("machine_reports_bp.category_view", category=category))
        
        # ๐ก ุชู ุงูุชุนุฏูู: ููุง ูุชู ุงุณุชุฏุนุงุก ุฏุงูุฉ ุงูุญูุธ ุงูุชู ุชุณุชุฎุฏู user_id
        _save_state(category, df=out_df)
        # ุฅุจุทุงู ุงููุงุด ูุถูุงู ุฅุนุงุฏุฉ ุจูุงุก ุงูููุงุฑุณ ูุน ุงูุจูุงูุงุช ุงูุฌุฏูุฏุฉ
        try:
            _invalidate_inquiry_cache(category)
        except Exception:
            pass
        
        msg = f"ุชู ุงุณุชูุฑุงุฏ ูุฏูุฌ {len(successful_files)} ููู(ุงุช) ุจูุฌุงุญ. ุฅุฌูุงูู ุงูุณุฌูุงุช ุจุนุฏ ุงูุฏูุฌ: {len(out_df)}"
        if failed_filenames:
             msg += f". ููุงุญุธุฉ: ูู ูุชู ุงุณุชุฎุฏุงู/ูุฑุงุกุฉ ุงููููุงุช ุงูุชุงููุฉ: {', '.join(failed_filenames)}"
             flash(msg, "warning")
        else:
             flash(msg, "success")
             
    except Exception as ex:
        current_app.logger.exception("Import error:")
        flash(f"ุฎุทุฃ ูู ุฏูุฌ ุงููููุงุช: {ex}", "danger")

    return redirect(url_for("machine_reports_bp.category_view", category=category))

@machine_reports_bp.route("/<category>/export", methods=["GET"])
@login_required
def export_excel(category):
    if category not in CATEGORIES:
        flash("ูุณู ุบูุฑ ููุฌูุฏ", "warning")
        return redirect(url_for("machine_reports_bp.index"))

    # ๐ก ุชู ุงูุชุนุฏูู: ููุง ูุชู ุงุณุชุฏุนุงุก ุฏุงูุฉ ุงูุชุญููู ุงูุชู ุชุณุชุฎุฏู user_id
    row = _load_state(category)
    if not row or not row.data_json:
        flash("ูุง ุชูุฌุฏ ุจูุงูุงุช ูุชุตุฏูุฑูุง. ุจุฑุฌุงุก ุงูุงุณุชูุฑุงุฏ ุฃููุงู ุจูุงุณุทุฉ ุงูุฃุฏูู.", "warning")
        return redirect(url_for("machine_reports_bp.category_view", category=category))

    df = _json_to_df(row.data_json)
    mapping = json.loads(row.mapping_json) if row and row.mapping_json else {}
    q = request.args.get("q", "", type=str)
    search_in = request.args.get("search_in", "all")

    out = _apply_mapping(df, mapping)
    search_cols_for_export = [search_in] if search_in != 'all' else None
    out = _filter_dataframe(out, q, search_cols=search_cols_for_export)
    out = _drop_empty_columns(out)
    out = _coerce_text_df(out)

    output = io.BytesIO()
    try:
        import xlsxwriter
        with pd.ExcelWriter(output, engine="xlsxwriter") as writer:
            sheet = "data"
            out.to_excel(writer, index=False, sheet_name=sheet)
            ws = writer.sheets[sheet]
            book = writer.book
            header_fmt = book.add_format({"bold": True, "bg_color": "#E2E8F0", "align": "center", "valign": "vcenter", "border": 1, "num_format": "@"})
            cell_fmt   = book.add_format({"align": "center", "valign": "vcenter", "border": 1, "num_format": "@"})
            for i, c in enumerate(out.columns):
                ws.write(0, i, c, header_fmt)
            n_rows, n_cols = len(out.index), len(out.columns)
            if n_rows > 0 and n_cols > 0:
                from xlsxwriter.utility import xl_rowcol_to_cell
                start = xl_rowcol_to_cell(1, 0); end = xl_rowcol_to_cell(n_rows, n_cols-1)
                ws.conditional_format(f"{start}:{end}", {"type":"no_blanks", "format": cell_fmt})
            for i, c in enumerate(out.columns):
                series = out[c].astype(str)
                width = min(max([len(str(c))] + [len(s) for s in series.tolist()]) + 2, 60)
                ws.set_column(i, i, width)
            ws.freeze_panes(1, 0)
        output.seek(0)
    except ModuleNotFoundError:
        try:
            from openpyxl.styles import Alignment, PatternFill, Font, Border, Side
            from openpyxl.utils import get_column_letter
            with pd.ExcelWriter(output, engine="openpyxl") as writer:
                sheet = "data"
                out.to_excel(writer, index=False, sheet_name=sheet)
                ws = writer.sheets[sheet]
                header_fill = PatternFill("solid", fgColor="E2E8F0")
                header_font = Font(bold=True)
                thin = Side(border_style="thin", color="CCCCCC")
                border = Border(left=thin, right=thin, top=thin, bottom=thin)
                center = Alignment(horizontal="center", vertical="center", wrap_text=False)
                for cell in ws[1]:
                    cell.fill = header_fill; cell.font = header_font
                    cell.alignment = center; cell.border = border; cell.number_format = "@"
                for row in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=ws.max_column):
                    for cell in row:
                        if cell.value not in (None, ""):
                            cell.alignment = center; cell.border = border; cell.number_format = "@"
                for i, c in enumerate(out.columns, start=1):
                    series = out[c].astype(str).tolist() if not out.empty else []
                    width = min(max([len(str(c))] + [len(s) for s in series]) + 2, 60) if series else len(str(c))
                    ws.column_dimensions[get_column_letter(i)].width = width
                ws.freeze_panes = "A2"
            output.seek(0)
        except Exception:
             # ูู ุญุงู ูุดู ุงูุงุณุชูุฑุงุฏ ูุงูุชูุณููุ ููุชูู ุจูุชุงุจุฉ ููู Excel ุจุฏูู ุชูุณูู
            with pd.ExcelWriter(output, engine="openpyxl") as writer:
                out.to_excel(writer, index=False, sheet_name="data")
            output.seek(0)


    filename = f"{CATEGORIES[category]}_export.xlsx"
    return send_file(output, as_attachment=True, download_name=filename,
                             mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

def _paginate(df: pd.DataFrame, page: int, page_size: int):
    """ูุธููุฉ ุงูุชูุณูู ููุตูุญุงุช"""
    n = len(df)
    start = max(0, (page - 1) * page_size)
    end = start + page_size
    return df.iloc[start:end], n

# ==================== (4) ูุธุงุฆู ุงูุฏูุฌ (Merge) ====================

_AR_DIGITS = str.maketrans("ููกูขูฃูคูฅูฆูงูจูฉ", "0123456789")
def _norm_key_text(s: str) -> str:
    if s is None:
        return ""
    t = str(s).translate(_AR_DIGITS)
    # ุชูุญูุฏ ุฃุดูุงู ุงูุฃููุ ุฅุฒุงูุฉ ุงูุชุทููู
    t = t.replace("\u0640", "").replace("ุฃ", "ุง").replace("ุฅ", "ุง").replace("ุข", "ุง")
    # ุชุญุณูู ุงูุชุทุจูุน ููุฃุญุฑู ุงูุดุงุฆุนุฉ ูู ุงูุฃุณูุงุก ุงูุนุฑุจูุฉ
    # ุชุญููู ุงููุงุก ุงูููุตูุฑุฉ ุฅูู ูุงุก ุนุงุฏูุฉุ ูุงูุชุงุก ุงููุฑุจูุทุฉ ุฅูู ูุงุก ูุชูููู ุงููุฑูู
    t = t.replace("ู", "ู").replace("ุฉ", "ู").replace("ุฆ", "ู").replace("ุค", "ู")
    # ุฅุฒุงูุฉ ุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ
    t = re.sub(r"\s+", " ", t).strip()
    return t

def _normalize_key_cols(df: pd.DataFrame, join_cols: list[str]) -> pd.DataFrame:
    out = df.copy()
    for c in join_cols:
        if c in out.columns:
            out[c] = out[c].map(lambda v: _norm_key_text("" if (pd.isna(v) or v is None) else str(v)))
        else:
            out[c] = ""
    return out

def _pick_entity_keys(category: str, dfs):
    """ูุญุฏุฏ ุงูููุงุชูุญ ุงูุฃูุซุฑ ุงุญุชูุงูุงู ููุฏูุฌ ุจูุงุกู ุนูู ุงูููุงุชูุญ ุงูููุฌูุฏุฉ ูู ุงููููุงุช ุงููุฑููุนุฉ."""
    
    # ูุงุฆูุฉ ุชูุถููุงุช ููุงุชูุญ ุงูุฏูุฌ ุจุชุฑุชูุจ ุงูุฃููููุฉ
    prefs_order = [("ุฑูู ุงูุนููู", "ุงุณู ุงูุนููู")]
    
    # ุฅุถุงูุฉ ููุงุชูุญ ุงููุฆุฉ ูุงูุงูุชุฑุงุถูุฉ ุงููุฏููุฉ ูุจุฏุงุฆูุ ูุน ุชุฌูุจ ุงูุชูุฑุงุฑ
    prefs_order.extend(ENTITY_KEYS.get(category, []))
    prefs_order.extend(ENTITY_KEYS["default"])
    
    # ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช ูุน ุงูุญูุงุธ ุนูู ุงูุชุฑุชูุจ
    seen = set()
    unique_prefs = []
    for pair in prefs_order:
        if pair not in seen:
            seen.add(pair)
            unique_prefs.append(pair)
    
    available = set()
    for d in dfs:
        available.update(list(d.columns)) 
        
    for a, b in unique_prefs:
        # ูุจุญุซ ุนู ุชุทุงุจู ุนูู ุงูุฃูู ูู ุฃุญุฏ ุงูููุชุงุญูู (ุงูุฑูู ุฃู ุงูุงุณู)
        keys = [k for k in [a, b] if k in available]
        if keys:
            return keys
            
    return []

def _pick_office_keys(dfs):
    """ุชุญุฏูุฏ ููุงุชูุญ ุงูุฏูุฌ ููุณุชูู ุงูููุชุจ/ุงูุฅุฏุงุฑุฉ ูุน ุฏุนู ุงููุฑุงุฏูุงุช ุงูุนุฑุจูุฉ ูุงูุฅูุฌููุฒูุฉ.
    ูุนูุฏ ูุงุฆูุฉ ุจุงูููุงุชูุญ ุงูููุฌูุฏุฉ ูุนูููุง (1 ุฃู 2) ุจุญุณุจ ุงูุฃุนูุฏุฉ ุงููุชุงุญุฉ.
    """
    office_pairs = [
        ("ุงูุงุฏุงุฑุฉ", "ุงูููุชุจ"),
        ("ุงูุฅุฏุงุฑุฉ", "ุงูููุชุจ"),
        ("ุงููุฑูุฒ", "ุงูููุชุจ"),
        ("ุงููุญุงูุธุฉ", "ุงูููุชุจ"),
        ("Administration", "Office"),
        ("Admin", "Office"),
        ("Department", "Office"),
        ("Branch", "Office"),
    ]

    # ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช ูุน ุงูุญูุงุธ ุนูู ุงูุชุฑุชูุจ
    seen = set(); unique_pairs = []
    for pair in office_pairs:
        if pair not in seen:
            seen.add(pair); unique_pairs.append(pair)

    available = set()
    for d in dfs:
        available.update(list(d.columns))

    # ุงุฎุชุฑ ุฃูู ุฒูุฌ ุชุชููุฑ ููู ููุงุชูุญ ูุนููุฉ ูู ุงูุจูุงูุงุช
    for a, b in unique_pairs:
        keys = [k for k in [a, b] if k in available]
        if keys:
            return keys
    # fallback: ุฃู ุนููุฏ ูุญุชูู ูููุฉ ููุชุจ/ุงุฏุงุฑุฉ
    office_like = [c for c in available if any(tok in str(c).lower() for tok in ["office","branch","ุงุฏุงุฑู","ุฅุฏุงุฑู","ุงูุงุฏุงุฑู","ุงูุฅุฏุงุฑุฉ","ููุชุจ"])]
    if office_like:
        return [office_like[0]]
    return []

def _read_any(file_storage) -> pd.DataFrame:
    if not file_storage or not file_storage.filename: return pd.DataFrame()
    name = file_storage.filename.lower()
    bio = io.BytesIO(file_storage.read())
    df = pd.DataFrame()
    try:
        if name.endswith((".xlsx",".xls")):
            df = pd.read_excel(bio, dtype=str)
        else:
            df = pd.read_csv(bio, dtype=str, encoding="utf-8", errors="ignore")
    except Exception:
        try:
            bio.seek(0); df = pd.read_excel(bio, dtype=str)
        except Exception:
            bio.seek(0); df = pd.read_csv(bio, dtype=str, encoding="utf-8", errors="ignore")
            
    return _coerce_text_df(df)


def _left_enrich(base: pd.DataFrame, data: pd.DataFrame, keys: list[str], suffix="__D") -> pd.DataFrame:
    if data is None or data.empty: 
        return base.copy()
    if not keys:
        return base.copy()

    # 1. ุงูุชูุธูู ูุชูุญูุฏ ุงูููุงุชูุญ
    baseN  = _normalize_key_cols(base, keys)
    dataN  = _normalize_key_cols(data, keys)

    # 2. ุฅุฒุงูุฉ ุงูุชูุฑุงุฑุงุช ูู ุจูุงูุงุช ุงูุฅุซุฑุงุก
    dataN = dataN.drop_duplicates(subset=keys, keep="first")

    # 3. ูุงุฆูุฉ ุงูุฃุนูุฏุฉ ุบูุฑ ุงูููุชุงุญูุฉ ูู ุจูุงูุงุช ุงูุฅุซุฑุงุก
    enrich_cols = [c for c in dataN.columns if c not in keys]

    # 4. ุฏูุฌ ุงูู DataFrame (ุณูุนุทู ุฃุณูุงุก ุฃุนูุฏุฉ ุจูุงุญูุฉ __D ููุฃุนูุฏุฉ ุงูููุฑุฑุฉ ุบูุฑ ุงูููุชุงุญูุฉ)
    merged = baseN.merge(dataN, how="left", on=keys, suffixes=("", suffix), copy=True)
    
    # 5. ุฏูุฌ ุงูุจูุงูุงุช ูู ุงูุฃุนูุฏุฉ ุฐุงุช ุงููุงุญูุฉ (Enrichment) ุฅูู ุงูุฃุนูุฏุฉ ุงูุฃุณุงุณูุฉ (Base) ุฅุฐุง ูุงูุช ูุงุฑุบุฉ
    for col in enrich_cols:
        col_with_suffix = f"{col}{suffix}"
        if col_with_suffix in merged.columns and col in merged.columns:
            lvals = merged[col].astype(str).map(_textify)
            rvals = merged[col_with_suffix].astype(str).map(_textify)
            
            merged[col] = lvals.where(lvals != "", rvals)
            
            merged.drop(columns=[col_with_suffix], inplace=True)
        elif col_with_suffix in merged.columns and col not in merged.columns:
             merged.rename(columns={col_with_suffix: col}, inplace=True)

    # 6. ุฅุฒุงูุฉ ุฃู ุฃุนูุฏุฉ ููุฑุฑุฉ ูุฏ ุชููู ุจููุช (ูุซูุงู ุฅุฐุง ูุงู ุงุณู ุงูุนููุฏ ูู BaseN ููุฑุฑูุง ุฃุตูุงู)
    return _coerce_text_df(merged)

def _append_unmatched(base: pd.DataFrame, data: pd.DataFrame, keys: list[str]) -> pd.DataFrame:
    if data is None or data.empty or not keys:
        return base.copy()

    baseK = _normalize_key_cols(base, keys)
    dataK = _normalize_key_cols(data, keys)

    base_keys = set(tuple(row) for row in baseK[keys].itertuples(index=False, name=None))
    data_only = dataK[~dataK[keys].apply(lambda r: tuple(r) in base_keys, axis=1)]

    if data_only.empty:
        return base.copy()

    union_cols = list(dict.fromkeys(list(base.columns) + [c for c in data_only.columns if c not in base.columns]))
    out = pd.concat([
        base.reindex(columns=union_cols),
        data_only.reindex(columns=union_cols)
    ], ignore_index=True)

    return _coerce_text_df(out)
    
def _apply_standard_transformations(df: pd.DataFrame) -> pd.DataFrame:
    """
    ุชุทุจูู ุงูุชุญูููุงุช ุงูููุงุณูุฉ ุงููุทููุจุฉ ุนูู ุงูุฃุนูุฏุฉ ูุจุงุดุฑุฉ ุจุนุฏ ุงูุงุณุชูุฑุงุฏ ูุงูุฏูุฌ.
    ูุซู ุชุญููู ููู (ูุงูููุฉ ุฑุฆูุณูุฉ/ูุฑุนูุฉ) ูู ุฑูููุฉ ุฅูู ูุตูุฉ.
    """
    if df.empty:
        return df

    out = df.copy()
    target_col = 'ูุงูููุฉ ุฑุฆูุณูุฉ/ูุฑุนูุฉ' 

    if target_col in out.columns:
        
        # ๐ก ุฏุงูุฉ ุงูุชุญููู: ุชุญููู ุงููููุฉ ุงูุฑูููุฉ 0/1 ุฅูู ูุต
        def _convert_primary_secondary(value):
            """ุชุญููู 0 ุฅูู ุฑุฆูุณูุฉ ู 1 ุฅูู ูุฑุนูุฉ."""
            # ูุถูู ุชุญููู ุงููููุฉ ุฅูู ูุต ูุธูู (ูุซู '0' ุฃู '1') ุญุชู ูู ูุงูุช ุฃุตููุง ุฑูู ุนุดุฑู (0.0ุ 1.0)
            value_str = _textify(value)
            
            if value_str == '0':
                return 'ุฑุฆูุณูุฉ'
            elif value_str == '1':
                return 'ูุฑุนูุฉ'
            # ูุชุฑู ุงููููุฉ ุงูุฃุตููุฉ ุฃู ูููุฉ ูุงุฑุบุฉ ุฅุฐุง ูู ุชูู 0 ุฃู 1
            return value_str 
            
        out[target_col] = out[target_col].map(_convert_primary_secondary)
        
    return out

def _merge_all(files: list[pd.DataFrame], category: str) -> pd.DataFrame:
    # ูุง ุชูู ุจุชุตููุฉ ุงูููุงุฆู ููุญูุงุธ ุนูู ุชุฑุชูุจ ุงููููุงุช (1..6)
    if not files or files[0] is None or files[0].empty:
        return pd.DataFrame()

    base = files[0].copy()
    
    def _has_keys(df: pd.DataFrame, keys: list[str]) -> bool:
        return keys and all(k in df.columns for k in keys)

    # ุงููููุงุช 2ุ 3ุ 4 โ ุฏูุฌ ุจููุงุชูุญ ููุงู ูุฑูุฉ (ุฑูู/ุงุณู ุงูุนููู ุฃู ุงููุฑุงุฏูุงุช) ุญุณุจ ุงููุฆุฉ
    for idx in [1, 2, 3]:
        if idx < len(files):
            df = files[idx]
            if df is not None and not df.empty:
                keys = _pick_entity_keys(category, [base, df])
                if _has_keys(base, keys) and _has_keys(df, keys):
                    base = _left_enrich(base, df, keys, suffix=f"__D{idx+1}")

    # ุงููููุงุช 5ุ 6 โ ุฏูุฌ ุจููุงุชูุญ ุงูุฅุฏุงุฑุฉ/ุงูููุชุจ ุจูุฑุงุฏูุงุช ูุฑูุฉ
    for idx in [4, 5]:
        if idx < len(files):
            df = files[idx]
            if df is not None and not df.empty:
                keys = _pick_office_keys([base, df])
                if _has_keys(base, keys) and _has_keys(df, keys):
                    base = _left_enrich(base, df, keys, suffix=f"__D{idx+1}")

    # ุชุทุจูู ุงูุชุญูููุงุช ุงูููุงุณูุฉ ุจุนุฏ ุงูุฏูุฌ
    base = _apply_standard_transformations(base)
    return _coerce_text_df(base)
# ==================== (1.a) ุชุณุฑูุน ุงูุงุณุชุนูุงู: ูุงุด ูููุงุฑุณ ูู ุงูุฐุงูุฑุฉ ====================
# ูุงุด ุงูุงุณุชุนูุงู ููู ุชุจููุจ (category): ูุญุชูุธ ุจูุณุฎุฉ DataFrame ุจุนุฏ ุงููุงุจูุฌ + ููุงุฑุณ ุณุฑูุนุฉ
INQUIRY_CACHE: dict[str, dict] = {}

def _mapping_signature(mapping: dict) -> str:
    try:
        import hashlib, json as _json
        s = _json.dumps(mapping or {}, ensure_ascii=False, sort_keys=True)
        return hashlib.sha1(s.encode('utf-8')).hexdigest()
    except Exception:
        return str(len(mapping or {}))

def _build_inquiry_cache(category: str):
    """ุฅูุดุงุก/ุชุญุฏูุซ ูุงุด ุงูุงุณุชุนูุงู ููุฆุฉ ูุนููุฉ: DataFrame ุจุนุฏ ุงููุงุจูุฌ + ููุงุฑุณ ููุจุญุซ."""
    row = _load_state(category)
    df = _json_to_df(row.data_json) if (row and row.data_json) else pd.DataFrame()
    mapping = json.loads(row.mapping_json) if (row and row.mapping_json) else {}
    mapped_df = _apply_mapping(df, mapping)
    mapped_df = _drop_empty_columns(mapped_df)

    indexes = {
        'code': {},          # ุฑูู ุงูุนููู/ุงููุฎุจุฒ/ุงูุชุงุฌุฑ/ุงููุงูููุฉ (ูุทุงุจูุฉ ูุงููุฉ)
        'serial': {},        # ุงููุณูุณู ููุชุฑุงุฏูุงุชู (ูุทุงุจูุฉ ูุงููุฉ)
        'name': {},          # ุงูุงุณู ุงููุงูู (ูุทุงุจูุฉ ูุงููุฉ)
        'machine_code': {},  # ุฑูู/ููุฏ ุงููุงูููุฉ (ูุทุงุจูุฉ ูุงููุฉ)
        # ูุทุงุจูุฉ ุฌุฒุฆูุฉ (ุจุงุฏุฆุฉ) ูุฃููุงุฏ ููุณูุณูุงุช ูููุฏ ุงููุงูููุฉ
        'code_prefix3': {}, 'code_prefix5': {},
        # ูุทุงุจูุฉ ุฌุฒุฆูุฉ (ููุงูุฉ/ูุงุญูุฉ) ููุฃููุงุฏ ูุชุณุฑูุน ุงูุจุญุซ ุจุงูุฃุฑูุงู ุงูุฃุฎูุฑุฉ
        'code_suffix3': {}, 'code_suffix5': {},
        'serial_prefix3': {}, 'serial_prefix5': {},
        'machine_code_prefix3': {}, 'machine_code_prefix5': {},
        # ููุฑุณุฉ ูููุงุช ุงูุงุณู ูุฏุนู ุงููุทุงุจูุฉ ุงูุฌุฒุฆูุฉ ุนูู ูุณุชูู ุงููููุงุช
        'name_token': {}, 'name_token_prefix3': {}, 'name_token_prefix5': {},
    }

    all_cols = list(mapped_df.columns)

    # ุฃุนูุฏุฉ ุงูููุฏ (ุฃููููุฉ + ุจุฏุงุฆู ุนุงูุฉ)
    code_cols = [c for c in ['ุฑูู ุงูุนููู', 'ุฑูู ุงููุฎุจุฒ', 'ุฑูู ุงูุชุงุฌุฑ', 'ุฑูู ุงููุงูููุฉ'] if c in all_cols]
    if not code_cols:
        code_cols = [c for c in all_cols if ('ุฑูู' in str(c) and 'ูุณูุณู' not in str(c) and 'ูููู' not in str(c))]

    # ุฃุนูุฏุฉ ุงููุณูุณู
    serial_candidates = ['ูุณูุณู ุงููุงูููุฉ', 'ูุณูุณู', 'serial', 'pos serial', 'sn', 'ุฑูู ุงููุงูููุฉ', 'machine serial']
    serial_cols = [c for c in all_cols if any(tok in str(c).lower() for tok in serial_candidates)]
    if not serial_cols:
        serial_cols = [c for c in all_cols if 'ูุณูุณู' in str(c)]

    # ุฃุนูุฏุฉ ุฑูู/ููุฏ ุงููุงูููุฉ
    mc_tokens = ['ุฑูู ุงููุงูููุฉ', 'ููุฏ ุงููุงูููุฉ', 'machine code', 'machine id', 'pos id', 'terminal id', 'ุฑูู ุงูุฌูุงุฒ', 'ููุฏ ุงูุฌูุงุฒ']
    mc_tokens = [t.lower() for t in mc_tokens]
    machine_code_cols = [c for c in all_cols if any(tok in str(c).lower() for tok in mc_tokens)]
    if not machine_code_cols:
        machine_code_cols = [c for c in all_cols if ('ุฑูู' in str(c) and 'ูุงู' in str(c))]
    if not machine_code_cols:
        machine_code_cols = [c for c in all_cols if ('code' in str(c).lower() and 'machine' in str(c).lower())]

    # ุฃุนูุฏุฉ ุงูุงุณู
    name_cols = [c for c in ['ุงุณู ุงูุนููู', 'ุงุณู ุงููุฎุจุฒ', 'ุงุณู ุงูุชุงุฌุฑ'] if c in all_cols]
    if not name_cols:
        name_cols = [c for c in all_cols if 'ุงุณู' in str(c)]

    # ุจูุงุก ุงูููุงุฑุณ: ูููุฉ ููุทุจูุนุฉ โ ูุงุฆูุฉ ููุงุฑุณ ุตููู
    for idx, r in mapped_df.iterrows():
        try:
            # code
            for c in code_cols:
                v = _textify(r.get(c))
                if v:
                    k = _norm_key_text(v)
                    if k:
                        indexes['code'].setdefault(k, []).append(idx)
                        # ุจุงุฏุฆุฉ 3 ู5
                        if len(k) >= 3:
                            p3 = k[:3]; indexes['code_prefix3'].setdefault(p3, []).append(idx)
                            s3 = k[-3:]; indexes['code_suffix3'].setdefault(s3, []).append(idx)
                        if len(k) >= 5:
                            p5 = k[:5]; indexes['code_prefix5'].setdefault(p5, []).append(idx)
                            s5 = k[-5:]; indexes['code_suffix5'].setdefault(s5, []).append(idx)
            # serial
            for c in serial_cols:
                v = _textify(r.get(c))
                if v:
                    k = _norm_key_text(v)
                    if k:
                        indexes['serial'].setdefault(k, []).append(idx)
                        if len(k) >= 3:
                            p3 = k[:3]; indexes['serial_prefix3'].setdefault(p3, []).append(idx)
                        if len(k) >= 5:
                            p5 = k[:5]; indexes['serial_prefix5'].setdefault(p5, []).append(idx)
            # machine_code
            for c in machine_code_cols:
                v = _textify(r.get(c))
                if v:
                    k = _norm_key_text(v)
                    if k:
                        indexes['machine_code'].setdefault(k, []).append(idx)
                        if len(k) >= 3:
                            p3 = k[:3]; indexes['machine_code_prefix3'].setdefault(p3, []).append(idx)
                        if len(k) >= 5:
                            p5 = k[:5]; indexes['machine_code_prefix5'].setdefault(p5, []).append(idx)
            # name
            for c in name_cols:
                v = _textify(r.get(c))
                if v:
                    k = _norm_key_text(v)
                    if k:
                        indexes['name'].setdefault(k, []).append(idx)
                        # ููุฑุณุฉ ูููุงุช ุงูุงุณู
                        tokens = [t for t in k.split(' ') if t]
                        for t in tokens:
                            indexes['name_token'].setdefault(t, []).append(idx)
                            if len(t) >= 3:
                                pt3 = t[:3]; indexes['name_token_prefix3'].setdefault(pt3, []).append(idx)
                            if len(t) >= 5:
                                pt5 = t[:5]; indexes['name_token_prefix5'].setdefault(pt5, []).append(idx)
        except Exception:
            # ูุชุฌุงูู ุฃู ุตู ูุณุจุจ ุฎุทุฃ ูู ุงูุชุทุจูุน/ุงูููุฑุณุฉ
            pass

    INQUIRY_CACHE[category] = {
        'df': mapped_df,
        'indexes': indexes,
        'state_id': getattr(row, 'id', None),
        'updated_at': getattr(row, 'updated_at', None),
        'mapping_signature': _mapping_signature(mapping),
        'cols': all_cols,
    }

def _get_inquiry_cache(category: str) -> dict:
    """ุฅุฑุฌุงุน ูุงุด ุตุงูุญุ ูุนูุฏ ุงูุจูุงุก ุฅุฐุง ูุงู ุบูุฑ ููุฌูุฏ ุฃู ูุฏูู."""
    row = _load_state(category)
    if not row or not row.data_json:
        return {'df': pd.DataFrame(), 'indexes': {'code':{},'serial':{},'name':{},'machine_code':{}}, 'cols': []}
    mapping = json.loads(row.mapping_json) if (row and row.mapping_json) else {}
    sig = _mapping_signature(mapping)
    cached = INQUIRY_CACHE.get(category)
    if (not cached) or (cached.get('state_id') != getattr(row, 'id', None)) or (cached.get('updated_at') != getattr(row, 'updated_at', None)) or (cached.get('mapping_signature') != sig):
        _build_inquiry_cache(category)
        cached = INQUIRY_CACHE.get(category)
    return cached or {
        'df': pd.DataFrame(),
        'indexes': {
            'code':{},'serial':{},'name':{},'machine_code':{},
            'code_prefix3':{},'code_prefix5':{},
            'code_suffix3':{},'code_suffix5':{},
            'serial_prefix3':{},'serial_prefix5':{},
            'machine_code_prefix3':{},'machine_code_prefix5':{},
            'name_token':{},'name_token_prefix3':{},'name_token_prefix5':{},
        },
        'cols': []
    }

def _invalidate_inquiry_cache(category: str):
    try:
        INQUIRY_CACHE.pop(category, None)
    except Exception:
        pass